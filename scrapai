#!/usr/bin/env python3
"""
scrapai - Simple Scrapy CLI for running Claude Code generated spiders with project support
"""

import argparse
import subprocess
import sys
import os
from pathlib import Path

def auto_activate_venv():
    """
    Auto-activate virtual environment by re-executing script with venv Python.
    This makes the CLI work transparently without requiring manual venv activation.
    """
    # Skip if already in venv or in special environments
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        return  # Already in venv

    if os.getenv('SKIP_VENV_CHECK') or os.getenv('AIRFLOW_HOME'):
        return  # Skip in Docker/Airflow

    # Look for venv in common locations
    script_dir = os.path.dirname(os.path.abspath(__file__))
    venv_paths = [
        os.path.join(script_dir, '.venv'),
        os.path.join(script_dir, 'venv'),
    ]

    for venv_path in venv_paths:
        venv_python = os.path.join(venv_path, 'bin', 'python')
        if os.path.exists(venv_python):
            # Re-execute this script with venv Python
            args = [venv_python] + sys.argv
            os.execv(venv_python, args)
            # execv replaces current process, so this line never runs
            return

    # No venv found - continue with system Python (setup will create one)

# Auto-activate venv before anything else (except for setup/verify commands)
if len(sys.argv) > 1 and sys.argv[1] not in ['setup', 'verify', '--help', '-h']:
    auto_activate_venv()

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Only import dependencies if not running setup command or if dependencies are available
try:
    from core.project_manager import ProjectManager
    from core.config_loader import ConfigLoader
except ImportError:
    if len(sys.argv) <= 1 or (sys.argv[1] not in ['setup', 'verify', '--help', '-h']):
        # If it's not setup/verify/help command and imports fail, show error
        print("‚ùå Dependencies not installed. Run './scrapai setup' first.")
        sys.exit(1)
    # For setup/verify/help command, continue without imports
    ProjectManager = None
    ConfigLoader = None

def main():
    
    parser = argparse.ArgumentParser(description='Run Scrapy spiders with project support')
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # projects command
    projects_parser = subparsers.add_parser('projects', help='Project management')
    projects_subparsers = projects_parser.add_subparsers(dest='projects_command', help='Project commands')
    
    # projects create
    create_parser = projects_subparsers.add_parser('create', help='Create a new project')
    create_parser.add_argument('--name', required=True, help='Project name')
    create_parser.add_argument('--spiders', required=True, help='Comma-separated spider names')
    
    # projects list
    projects_subparsers.add_parser('list', help='List all projects')
    
    # projects status
    status_parser = projects_subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # projects delete
    delete_parser = projects_subparsers.add_parser('delete', help='Delete a project')
    delete_parser.add_argument('--project', required=True, help='Project name')
    
    # crawl command
    crawl_parser = subparsers.add_parser('crawl', help='Run a spider')
    crawl_parser.add_argument('--project', help='Project name (optional for DB spiders)')
    crawl_parser.add_argument('spider', help='Spider name to run')
    crawl_parser.add_argument('--output', '-o', help='Output file path (optional, uses project config)')
    crawl_parser.add_argument('--limit', '-l', type=int, help='Limit number of items')
    crawl_parser.add_argument('--timeout', '-t', type=int, help='Max runtime in seconds (graceful stop, e.g. 28800 for 8h)')
    
    # crawl-all command
    crawl_all_parser = subparsers.add_parser('crawl-all', help='Run all spiders in a project')
    crawl_all_parser.add_argument('--project', required=True, help='Project name')
    crawl_all_parser.add_argument('--limit', '-l', type=int, help='Limit number of items per spider')
    
    # list command
    list_parser = subparsers.add_parser('list', help='List available spiders')
    list_parser.add_argument('--project', help='Project name (optional)')
    
    # test command
    test_parser = subparsers.add_parser('test', help='Test a spider with limited items')
    test_parser.add_argument('--project', required=True, help='Project name')
    test_parser.add_argument('spider', help='Spider name to test')
    test_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of items to test')
    
    # status command
    status_parser = subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # spiders command
    spiders_parser = subparsers.add_parser('spiders', help='Spider management')
    spiders_subparsers = spiders_parser.add_subparsers(dest='spiders_command', help='Spider commands')
    
    # spiders list
    list_spiders_parser = spiders_subparsers.add_parser('list', help='List all spiders in DB')
    list_spiders_parser.add_argument('--project', help='Filter by project name (default: show all)')
    
    # spiders import
    import_parser = spiders_subparsers.add_parser('import', help='Import spider from JSON')
    import_parser.add_argument('file', help='JSON file containing spider definition (use "-" to read from stdin)')
    import_parser.add_argument('--project', default='default', help='Project name (default: default)')
    
    # spiders delete
    delete_parser = spiders_subparsers.add_parser('delete', help='Delete a spider')
    delete_parser.add_argument('name', help='Spider name')
    delete_parser.add_argument('--project', help='Project name (optional - if not specified, deletes first match)')
    delete_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # queue command for queue management
    queue_parser = subparsers.add_parser('queue', help='Queue management')
    queue_subparsers = queue_parser.add_subparsers(dest='queue_command', help='Queue commands')

    # queue add
    add_parser = queue_subparsers.add_parser('add', help='Add website to queue')
    add_parser.add_argument('url', help='Website URL to add to queue')
    add_parser.add_argument('-m', '--message', dest='custom_instruction', help='Custom instruction for processing')
    add_parser.add_argument('--priority', type=int, default=5, help='Priority (higher = sooner, default: 5)')
    add_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue list
    list_parser = queue_subparsers.add_parser('list', help='List queue items (excludes failed/completed by default)')
    list_parser.add_argument('--project', default='default', help='Project name (default: default)')
    list_parser.add_argument('--status', help='Filter by status (pending, processing, completed, failed)')
    list_parser.add_argument('--limit', type=int, default=5, help='Limit number of items to show (default: 5)')
    list_parser.add_argument('--all', action='store_true', help='Show all items including failed and completed')
    list_parser.add_argument('--count', action='store_true', help='Show only the count of items')

    # queue next
    next_parser = queue_subparsers.add_parser('next', help='Get next item from queue (atomic claim)')
    next_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue complete
    complete_parser = queue_subparsers.add_parser('complete', help='Mark item as completed')
    complete_parser.add_argument('id', type=int, help='Queue item ID')

    # queue fail
    fail_parser = queue_subparsers.add_parser('fail', help='Mark item as failed')
    fail_parser.add_argument('id', type=int, help='Queue item ID')
    fail_parser.add_argument('-m', '--message', dest='error_message', help='Error message')

    # queue retry
    retry_parser = queue_subparsers.add_parser('retry', help='Retry a failed item')
    retry_parser.add_argument('id', type=int, help='Queue item ID')

    # queue remove
    remove_parser = queue_subparsers.add_parser('remove', help='Remove item from queue')
    remove_parser.add_argument('id', type=int, help='Queue item ID')

    # queue cleanup
    cleanup_parser = queue_subparsers.add_parser('cleanup', help='Bulk cleanup queue items')
    cleanup_parser.add_argument('--completed', action='store_true', help='Remove all completed items')
    cleanup_parser.add_argument('--failed', action='store_true', help='Remove all failed items')
    cleanup_parser.add_argument('--all', action='store_true', help='Remove all completed and failed items')
    cleanup_parser.add_argument('--project', default='default', help='Project name (default: default)')
    cleanup_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # queue bulk
    bulk_parser = queue_subparsers.add_parser('bulk', help='Bulk add URLs from JSON or CSV file')
    bulk_parser.add_argument('file', help='JSON or CSV file containing URLs (JSON: array of objects with "url" field, CSV: columns "url", optional "name"/"custom_instruction", "priority")')
    bulk_parser.add_argument('--project', default='default', help='Project name (default: default)')
    bulk_parser.add_argument('--priority', type=int, default=5, help='Default priority for items without priority column (default: 5)')

    # db command for database operations
    db_parser = subparsers.add_parser('db', help='Database management')
    db_subparsers = db_parser.add_subparsers(dest='db_command', help='Database commands')

    # db migrate
    db_subparsers.add_parser('migrate', help='Run database migrations')

    # db current
    db_subparsers.add_parser('current', help='Show current migration revision')

    # show command
    show_parser = subparsers.add_parser('show', help='Show scraped articles from database')
    show_parser.add_argument('spider_name', help='Spider name to show articles from')
    show_parser.add_argument('--project', help='Project name (optional - if not specified, shows first match)')
    show_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of articles to show (default: 5)')
    show_parser.add_argument('--url', help='Show only articles containing this URL pattern')
    show_parser.add_argument('--text', '-t', help='Show only articles containing this text in title or content')
    show_parser.add_argument('--title', help='Show only articles containing this text in title')

    # export command
    export_parser = subparsers.add_parser('export', help='Export scraped articles from database')
    export_parser.add_argument('spider_name', help='Spider name to export articles from')
    export_parser.add_argument('--project', help='Project name (optional - if not specified, exports first match)')
    export_parser.add_argument('--format', '-f', choices=['csv', 'json', 'jsonl', 'parquet'], required=True, help='Export format (csv, json, jsonl, or parquet)')
    export_parser.add_argument('--output', '-o', help='Output file path (default: data/<spider_name>_export.<format>)')
    export_parser.add_argument('--limit', '-l', type=int, help='Limit number of articles to export')
    export_parser.add_argument('--url', help='Export only articles containing this URL pattern')
    export_parser.add_argument('--text', '-t', help='Export only articles containing this text in title or content')
    export_parser.add_argument('--title', help='Export only articles containing this text in title')

    # extract-urls command
    extract_urls_parser = subparsers.add_parser('extract-urls', help='Extract all URLs from an HTML file')
    extract_urls_parser.add_argument('--file', type=str, required=True, help='Path to HTML file')
    extract_urls_parser.add_argument('--output', '-o', type=str, help='Output file path (optional, prints to stdout if not specified)')

    # setup command
    subparsers.add_parser('setup', help='Setup virtual environment and database')

    # verify command
    subparsers.add_parser('verify', help='Verify environment setup (no installations)')

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Handle commands
    if args.command == 'setup':
        handle_setup_command()
    elif args.command == 'verify':
        handle_verify_command()
    elif args.command == 'projects':
        handle_projects_command(args)
    elif args.command == 'spiders':
        handle_spiders_command(args)
    elif args.command == 'queue':
        handle_queue_command(args)
    elif args.command == 'db':
        handle_db_command(args)
    elif args.command == 'show':
        handle_show_command(args)
    elif args.command == 'export':
        handle_export_command(args)
    elif args.command == 'extract-urls':
        handle_extract_urls_command(args)
    elif args.command == 'crawl':
        run_spider(args.project, args.spider, args.output, args.limit, args.timeout)
    elif args.command == 'crawl-all':
        run_all_spiders(args.project, args.limit)
    elif args.command == 'list':
        list_spiders(args.project)
    elif args.command == 'test':
        test_spider(args.project, args.spider, args.limit)
    elif args.command == 'status':
        show_project_status(args.project)
    elif args.command == 'logs':
        show_project_logs(args.project, args.spider)

def handle_setup_command():
    """Handle setup command - automated virtual environment and database setup"""
    import subprocess
    import sys
    from pathlib import Path

    print("üöÄ Setting up ScrapAI environment...")

    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Check if virtual environment exists
    venv_path = Path('.venv')
    venv_python = venv_path / 'bin' / 'python'

    if not venv_path.exists():
        print("üì¶ Creating virtual environment...")
        try:
            subprocess.run([sys.executable, '-m', 'venv', '.venv'], check=True, cwd=script_dir)
            print("‚úÖ Virtual environment created")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to create virtual environment: {e}")
            sys.exit(1)
    else:
        print("‚úÖ Virtual environment already exists")

    # Install requirements using venv Python directly (no activation needed!)
    requirements_path = Path('requirements.txt')
    if requirements_path.exists():
        print("üìã Installing requirements...")
        try:
            # Use venv's Python directly - no activation needed
            subprocess.run([str(venv_python), '-m', 'pip', 'install', '--upgrade', 'pip'],
                         check=True, cwd=script_dir)
            subprocess.run([str(venv_python), '-m', 'pip', 'install', '-r', 'requirements.txt'],
                         check=True, cwd=script_dir)
            print("‚úÖ Requirements installed")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install requirements: {e}")
            sys.exit(1)
    else:
        print("‚ö†Ô∏è  requirements.txt not found")

    # Check database setup using venv Python
    print("üóÑÔ∏è  Initializing database...")
    try:
        init_db_path = os.path.join(script_dir, 'init_db.py')
        subprocess.run([str(venv_python), init_db_path], check=True, cwd=script_dir)
        print("‚úÖ Database initialized")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Database setup failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Setup error: {e}")
        sys.exit(1)

    print("üéâ ScrapAI setup complete!")
    print("üìù You can now:")
    print("   ‚Ä¢ List spiders: ./scrapai spiders list")
    print("   ‚Ä¢ Import spiders: ./scrapai spiders import <file>")
    print("   ‚Ä¢ Run crawls: ./scrapai crawl <spider_name>")

def handle_verify_command():
    """Verify environment setup without installing anything"""
    import subprocess
    import sys
    from pathlib import Path

    print("üîç Verifying ScrapAI environment...\n")

    all_good = True
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Check if virtual environment exists
    venv_path = Path('.venv')
    venv_python = venv_path / 'bin' / 'python'

    if not venv_path.exists():
        print("‚ùå Virtual environment not found")
        print("   Run: ./scrapai setup")
        all_good = False
    else:
        print("‚úÖ Virtual environment exists")

        # Check if key dependencies are installed (using venv Python directly)
        try:
            result = subprocess.run(
                [str(venv_python), '-c', 'import scrapy, sqlalchemy, alembic; print("ok")'],
                capture_output=True,
                text=True,
                cwd=script_dir
            )
            if result.returncode == 0 and 'ok' in result.stdout:
                print("‚úÖ Core dependencies installed")
            else:
                print("‚ùå Missing dependencies")
                print("   Run: ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"‚ùå Error checking dependencies: {e}")
            print("   Run: ./scrapai setup")
            all_good = False

    # Check database setup
    if all_good:
        try:
            result = subprocess.run(
                [str(venv_python), '-m', 'alembic', 'current'],
                capture_output=True,
                text=True,
                cwd=script_dir
            )
            if result.returncode == 0:
                if 'head' in result.stdout or result.stdout.strip():
                    print("‚úÖ Database initialized")
                else:
                    print("‚ùå Database not initialized")
                    print("   Run: ./scrapai setup")
                    all_good = False
            else:
                print("‚ùå Unable to check database status")
                print("   Run: ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"‚ùå Error checking database: {e}")
            print("   Run: ./scrapai setup")
            all_good = False

    print()
    if all_good:
        print("üéâ Environment is ready!")
        print("üìù You can now:")
        print("   ‚Ä¢ List spiders: ./scrapai spiders list --project <name>")
        print("   ‚Ä¢ Import spiders: ./scrapai spiders import <file> --project <name>")
        print("   ‚Ä¢ Run crawls: ./scrapai crawl <spider_name> --project <name>")
    else:
        print("‚ö†Ô∏è  Environment setup incomplete")
        print("   Run: ./scrapai setup")

def handle_spiders_command(args):
    """Handle spider management commands"""
    from core.db import get_db
    from core.models import Spider, SpiderRule, SpiderSetting
    import json
    
    db = next(get_db())
    
    if args.spiders_command == 'list':
        query = db.query(Spider)
        if hasattr(args, 'project') and args.project:
            query = query.filter(Spider.project == args.project)
            print(f"üìã Available Spiders (DB) - Project: {args.project}:")
        else:
            print("üìã Available Spiders (DB) - All Projects:")

        spiders = query.all()
        if spiders:
            for s in spiders:
                created = s.created_at.strftime('%Y-%m-%d %H:%M') if s.created_at else 'Unknown'
                updated = s.updated_at.strftime('%Y-%m-%d %H:%M') if hasattr(s, 'updated_at') and s.updated_at else created
                project_tag = f"[{s.project}]" if hasattr(s, 'project') and s.project else "[default]"
                source_url = s.source_url if hasattr(s, 'source_url') and s.source_url else None

                print(f"  ‚Ä¢ {s.name} {project_tag} (Active: {s.active}) - Created: {created}, Updated: {updated}")
                if source_url:
                    print(f"    Source: {source_url}")
        else:
            if hasattr(args, 'project') and args.project:
                print(f"No spiders found in project '{args.project}'.")
            else:
                print("No spiders found in database.")
            
    elif args.spiders_command == 'import':
        try:
            if args.file == '-':
                # Read from stdin
                import sys
                data = json.load(sys.stdin)
            else:
                # Read from file
                with open(args.file, 'r') as f:
                    data = json.load(f)
            
            # Get project name from args
            project_name = args.project if hasattr(args, 'project') else 'default'

            # Validate source_url is provided
            if 'source_url' not in data or not data['source_url']:
                print(f"‚ùå Error: 'source_url' is required in spider JSON")
                print(f"   Add the original website URL: \"source_url\": \"https://example.com\"")
                return

            # Check if spider exists
            existing = db.query(Spider).filter(Spider.name == data['name']).first()
            if existing:
                print(f"‚ö†Ô∏è  Spider '{data['name']}' already exists. Updating...")
                # Simple update: delete and recreate (or just update fields)
                # For now, let's just update the main fields and replace rules
                existing.allowed_domains = data['allowed_domains']
                existing.start_urls = data['start_urls']
                existing.source_url = data.get('source_url')  # Optional field
                existing.project = project_name

                # Clear old rules and settings
                db.query(SpiderRule).filter(SpiderRule.spider_id == existing.id).delete()
                db.query(SpiderSetting).filter(SpiderSetting.spider_id == existing.id).delete()
                spider = existing
            else:
                spider = Spider(
                    name=data['name'],
                    allowed_domains=data['allowed_domains'],
                    start_urls=data['start_urls'],
                    source_url=data.get('source_url'),  # Optional field
                    project=project_name
                )
                db.add(spider)
                db.flush() # Get ID
            
            # Add Rules
            for r_data in data.get('rules', []):
                rule = SpiderRule(
                    spider_id=spider.id,
                    allow_patterns=r_data.get('allow'),
                    deny_patterns=r_data.get('deny'),
                    restrict_xpaths=r_data.get('restrict_xpaths'),
                    restrict_css=r_data.get('restrict_css'),
                    callback=r_data.get('callback'),
                    follow=r_data.get('follow', True),
                    priority=r_data.get('priority', 0)
                )
                db.add(rule)
            
            # Add Settings
            for k, v in data.get('settings', {}).items():
                setting = SpiderSetting(
                    spider_id=spider.id,
                    key=k,
                    value=str(v),
                    type=type(v).__name__
                )
                db.add(setting)
            
            db.commit()
            print(f"‚úÖ Spider '{spider.name}' imported successfully!")
            
        except Exception as e:
            db.rollback()
            print(f"‚ùå Error importing spider: {e}")
            
    elif args.spiders_command == 'delete':
        query = db.query(Spider).filter(Spider.name == args.name)

        # Filter by project if specified
        if hasattr(args, 'project') and args.project:
            query = query.filter(Spider.project == args.project)
            project_msg = f" in project '{args.project}'"
        else:
            project_msg = ""

        spider = query.first()

        if spider:
            if not args.force:
                confirm = input(f"Are you sure you want to delete spider '{args.name}'{project_msg}? (y/N): ")
                if confirm.lower() != 'y':
                    print("‚ùå Delete cancelled")
                    return

            db.delete(spider)
            db.commit()
            print(f"üóëÔ∏è  Spider '{args.name}'{project_msg} deleted!")
        else:
            print(f"‚ùå Spider '{args.name}'{project_msg} not found.")

def handle_queue_command(args):
    """Handle queue management commands"""
    from core.db import get_db
    from core.models import CrawlQueue
    from datetime import datetime
    import socket
    import getpass
    from sqlalchemy import text

    db = next(get_db())

    if args.queue_command == 'add':
        # Check if URL already exists in queue for this project
        existing = db.query(CrawlQueue).filter(
            CrawlQueue.project_name == args.project,
            CrawlQueue.website_url == args.url
        ).first()

        if existing:
            status_emoji = {
                'pending': '‚è≥',
                'processing': 'üîÑ',
                'completed': '‚úÖ',
                'failed': '‚ùå'
            }.get(existing.status, '‚ùì')

            print(f"‚ö†Ô∏è  URL already exists in queue")
            print(f"   {status_emoji} ID: {existing.id}")
            print(f"   Status: {existing.status}")
            print(f"   URL: {existing.website_url}")
            print(f"   Skipping duplicate...")
            return

        # Add new item to queue
        queue_item = CrawlQueue(
            project_name=args.project,
            website_url=args.url,
            custom_instruction=args.custom_instruction,
            priority=args.priority
        )
        db.add(queue_item)
        db.commit()

        print(f"‚úÖ Added to queue (ID: {queue_item.id})")
        print(f"   URL: {args.url}")
        print(f"   Project: {args.project}")
        print(f"   Priority: {args.priority}")
        if args.custom_instruction:
            print(f"   Instructions: {args.custom_instruction}")

    elif args.queue_command == 'list':
        # List queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.status:
            # If status is explicitly specified, filter by that status
            query = query.filter(CrawlQueue.status == args.status)
        elif not args.all:
            # By default, exclude failed and completed items (show only pending and processing)
            query = query.filter(CrawlQueue.status.in_(['pending', 'processing']))

        # If --count flag is set, just return the count
        if args.count:
            count = query.count()
            status_msg = f" ({args.status})" if args.status else ""
            print(f"{count}")
            return

        query = query.order_by(CrawlQueue.priority.desc(), CrawlQueue.created_at.asc())

        if args.limit:
            query = query.limit(args.limit)

        items = query.all()

        if not items:
            status_msg = f" with status '{args.status}'" if args.status else ""
            print(f"üìã No items in queue for project '{args.project}'{status_msg}")
            return

        print(f"üìã Queue for project '{args.project}':")
        print()

        for item in items:
            status_emoji = {
                'pending': '‚è≥',
                'processing': 'üîÑ',
                'completed': '‚úÖ',
                'failed': '‚ùå'
            }.get(item.status, '‚ùì')

            print(f"{status_emoji} [{item.id}] {item.website_url}")
            print(f"   Status: {item.status} | Priority: {item.priority}")

            if item.custom_instruction:
                print(f"   Instructions: {item.custom_instruction}")

            if item.processing_by:
                locked_time = item.locked_at.strftime('%Y-%m-%d %H:%M') if item.locked_at else 'Unknown'
                print(f"   Processing by: {item.processing_by} (since {locked_time})")

            if item.error_message:
                print(f"   Error: {item.error_message}")

            if item.completed_at:
                completed_time = item.completed_at.strftime('%Y-%m-%d %H:%M')
                print(f"   Completed: {completed_time}")

            print()

    elif args.queue_command == 'next':
        # Atomic claim next item (supports both PostgreSQL and SQLite)
        from core.db import is_postgres
        processing_by = f"{getpass.getuser()}@{socket.gethostname()}"

        if is_postgres():
            # PostgreSQL: Use FOR UPDATE SKIP LOCKED for best performance
            result = db.execute(text("""
                UPDATE crawl_queue
                SET
                    status = 'processing',
                    processing_by = :processing_by,
                    locked_at = NOW(),
                    updated_at = NOW()
                WHERE id = (
                    SELECT id
                    FROM crawl_queue
                    WHERE status = 'pending'
                      AND project_name = :project_name
                    ORDER BY priority DESC, created_at ASC
                    LIMIT 1
                    FOR UPDATE SKIP LOCKED
                )
                RETURNING id, website_url, custom_instruction, priority
            """), {
                'processing_by': processing_by,
                'project_name': args.project
            })
        else:
            # SQLite: Use optimistic locking (re-check status in WHERE clause)
            result = db.execute(text("""
                UPDATE crawl_queue
                SET
                    status = 'processing',
                    processing_by = :processing_by,
                    locked_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = (
                    SELECT id
                    FROM crawl_queue
                    WHERE status = 'pending'
                      AND project_name = :project_name
                    ORDER BY priority DESC, created_at ASC
                    LIMIT 1
                )
                AND status = 'pending'
                RETURNING id, website_url, custom_instruction, priority
            """), {
                'processing_by': processing_by,
                'project_name': args.project
            })

        db.commit()

        row = result.fetchone()

        if row:
            print(f"üîÑ Claimed item from queue:")
            print(f"   ID: {row[0]}")
            print(f"   URL: {row[1]}")
            if row[2]:
                print(f"   Instructions: {row[2]}")
            print(f"   Priority: {row[3]}")
            print(f"   Locked by: {processing_by}")
        else:
            print(f"üì≠ No pending items in queue for project '{args.project}'")

    elif args.queue_command == 'complete':
        # Mark item as completed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        now = datetime.now(timezone.utc)
        item.status = 'completed'
        item.completed_at = now
        item.updated_at = now
        db.commit()

        print(f"‚úÖ Item {args.id} marked as completed")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'fail':
        # Mark item as failed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'failed'
        item.error_message = args.error_message
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"‚ùå Item {args.id} marked as failed")
        print(f"   URL: {item.website_url}")
        if args.error_message:
            print(f"   Error: {args.error_message}")

    elif args.queue_command == 'retry':
        # Retry a failed item
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'pending'
        item.retry_count += 1
        item.error_message = None
        item.processing_by = None
        item.locked_at = None
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"üîÑ Item {args.id} reset to pending (retry count: {item.retry_count})")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'remove':
        # Remove item from queue
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        url = item.website_url
        db.delete(item)
        db.commit()

        print(f"üóëÔ∏è  Item {args.id} removed from queue")
        print(f"   URL: {url}")

    elif args.queue_command == 'cleanup':
        # Bulk cleanup queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.all:
            # Remove all completed and failed items
            query = query.filter(CrawlQueue.status.in_(['completed', 'failed']))
        elif args.completed:
            # Remove only completed items
            query = query.filter(CrawlQueue.status == 'completed')
        elif args.failed:
            # Remove only failed items
            query = query.filter(CrawlQueue.status == 'failed')
        else:
            print("‚ùå Please specify --completed, --failed, or --all")
            return

        items = query.all()

        if not items:
            status_filter = "all completed and failed" if args.all else ("completed" if args.completed else "failed")
            print(f"üìã No {status_filter} items to cleanup in project '{args.project}'")
            return

        # Show what will be deleted
        print(f"üóëÔ∏è  Found {len(items)} items to remove:")
        for item in items[:5]:  # Show first 5
            status_emoji = '‚úÖ' if item.status == 'completed' else '‚ùå'
            print(f"   {status_emoji} [{item.id}] {item.website_url}")
        if len(items) > 5:
            print(f"   ... and {len(items) - 5} more")

        # Confirm unless --force
        if not args.force:
            confirm = input(f"\nRemove {len(items)} items? (y/N): ")
            if confirm.lower() != 'y':
                print("‚ùå Cleanup cancelled")
                return

        # Delete items
        for item in items:
            db.delete(item)
        db.commit()

        print(f"‚úÖ Removed {len(items)} items from queue")

    elif args.queue_command == 'bulk':
        # Bulk add URLs from JSON or CSV file
        import json
        import csv
        from pathlib import Path

        file_path = Path(args.file)

        try:
            # Detect file format by extension
            if file_path.suffix.lower() == '.csv':
                # Parse CSV file
                with open(args.file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    data = list(reader)

                    # Validate CSV has 'url' column
                    if not data or 'url' not in data[0]:
                        print("‚ùå CSV file must have a 'url' column")
                        return

            elif file_path.suffix.lower() == '.json':
                # Parse JSON file
                with open(args.file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if not isinstance(data, list):
                    print("‚ùå JSON file must contain an array of objects")
                    return
            else:
                print(f"‚ùå Unsupported file format: {file_path.suffix}")
                print("   Supported formats: .json, .csv")
                return

        except FileNotFoundError:
            print(f"‚ùå File not found: {args.file}")
            return
        except json.JSONDecodeError as e:
            print(f"‚ùå Invalid JSON: {e}")
            return
        except csv.Error as e:
            print(f"‚ùå Invalid CSV: {e}")
            return
        except Exception as e:
            print(f"‚ùå Error reading file: {e}")
            return

        added = 0
        skipped = 0

        for item in data:
            url = item.get('url')
            if not url:
                print(f"‚ö†Ô∏è  Skipping item without URL: {item}")
                skipped += 1
                continue

            # Check if URL already exists in queue for this project
            existing = db.query(CrawlQueue).filter(
                CrawlQueue.project_name == args.project,
                CrawlQueue.website_url == url
            ).first()

            if existing:
                skipped += 1
                continue

            # Get custom instruction from 'name' or 'custom_instruction' field
            custom_instruction = item.get('custom_instruction') or item.get('name')

            # Get priority from item or use default
            item_priority = item.get('priority')
            if item_priority is not None:
                try:
                    item_priority = int(item_priority)
                except (ValueError, TypeError):
                    item_priority = args.priority
            else:
                item_priority = args.priority

            # Add new item to queue
            queue_item = CrawlQueue(
                project_name=args.project,
                website_url=url,
                custom_instruction=custom_instruction,
                priority=item_priority
            )
            db.add(queue_item)
            added += 1

        db.commit()

        print(f"‚úÖ Bulk add complete:")
        print(f"   Added: {added}")
        print(f"   Skipped (duplicates/invalid): {skipped}")
        print(f"   Project: {args.project}")
        print(f"   Format: {file_path.suffix.upper()}")

def handle_db_command(args):
    """Handle database management commands"""
    import subprocess
    import sys
    import os
    
    if args.db_command == 'migrate':
        print("üîÑ Running database migrations...")
        try:
            result = subprocess.run([
                sys.executable, '-m', 'alembic', 'upgrade', 'head'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
            if result.returncode == 0:
                print("‚úÖ Migrations completed successfully!")
            else:
                print("‚ùå Migration failed!")
        except Exception as e:
            print(f"‚ùå Error running migrations: {e}")
            
    elif args.db_command == 'current':
        try:
            subprocess.run([
                sys.executable, '-m', 'alembic', 'current'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
        except Exception as e:
            print(f"‚ùå Error checking current revision: {e}")

def handle_show_command(args):
    """Show scraped articles from database"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from datetime import datetime

    db = next(get_db())

    # Find spider by name
    query = db.query(Spider).filter(Spider.name == args.spider_name)

    # Filter by project if specified
    if hasattr(args, 'project') and args.project:
        query = query.filter(Spider.project == args.project)
        project_msg = f" in project '{args.project}'"
    else:
        project_msg = ""

    spider = query.first()
    if not spider:
        print(f"‚ùå Spider '{args.spider_name}'{project_msg} not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()

    if not items:
        print(f"üì≠ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    print(f"üì∞ Showing {len(items)} articles from '{args.spider_name}':")
    if filters_applied:
        print(f"   (filtered by: {', '.join(filters_applied)})")
    print()

    for i, item in enumerate(items, 1):
        # Format date nicely
        scraped_date = item.scraped_at.strftime('%Y-%m-%d %H:%M') if item.scraped_at else 'Unknown'
        pub_date = item.published_date.strftime('%Y-%m-%d') if item.published_date else 'Unknown'

        print(f"üî∏ [{i}] {item.title or 'No Title'}")
        print(f"   üìÖ Published: {pub_date} | Scraped: {scraped_date}")
        print(f"   üîó {item.url}")
        if item.author:
            print(f"   ‚úçÔ∏è  {item.author}")

        # Show content preview (first 150 chars)
        if item.content:
            content_preview = item.content[:150].replace('\n', ' ').strip()
            if len(item.content) > 150:
                content_preview += "..."
            print(f"   üìù {content_preview}")
        print()

def handle_export_command(args):
    """Export scraped articles from database to various formats"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from pathlib import Path
    import csv
    import json

    db = next(get_db())

    # Find spider by name
    query = db.query(Spider).filter(Spider.name == args.spider_name)

    # Filter by project if specified
    if hasattr(args, 'project') and args.project:
        query = query.filter(Spider.project == args.project)
        project_msg = f" in project '{args.project}'"
    else:
        project_msg = ""

    spider = query.first()
    if not spider:
        print(f"‚ùå Spider '{args.spider_name}'{project_msg} not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    if args.limit:
        items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()
    else:
        items = query.order_by(ScrapedItem.scraped_at.desc()).all()

    if not items:
        print(f"üì≠ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    # Determine output file path
    if args.output:
        output_path = Path(args.output)
    else:
        # Create data directory if it doesn't exist
        from datetime import datetime
        data_dir = Path('data')
        data_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%d%m%Y_%H%M%S')
        output_path = data_dir / f"{args.spider_name}_export_{timestamp}.{args.format}"

    # Ensure parent directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert items to list of dicts
    items_data = []
    for item in items:
        item_dict = {
            'id': item.id,
            'url': item.url,
            'title': item.title,
            'content': item.content,
            'author': item.author,
            'published_date': item.published_date.isoformat() if item.published_date else None,
            'scraped_at': item.scraped_at.isoformat() if item.scraped_at else None,
            'metadata': item.metadata_json
        }
        items_data.append(item_dict)

    try:
        # Export based on format
        if args.format == 'csv':
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                if items_data:
                    writer = csv.DictWriter(f, fieldnames=items_data[0].keys())
                    writer.writeheader()
                    writer.writerows(items_data)
            print(f"‚úÖ Exported {len(items_data)} articles to CSV: {output_path}")

        elif args.format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(items_data, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ Exported {len(items_data)} articles to JSON: {output_path}")

        elif args.format == 'jsonl':
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in items_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"‚úÖ Exported {len(items_data)} articles to JSONL: {output_path}")

        elif args.format == 'parquet':
            try:
                import pandas as pd
                df = pd.DataFrame(items_data)
                df.to_parquet(output_path, index=False)
                print(f"‚úÖ Exported {len(items_data)} articles to Parquet: {output_path}")
            except ImportError:
                print("‚ùå Parquet export requires pandas and pyarrow libraries.")
                print("   Run: pip install pandas pyarrow")
                return

        if filters_applied:
            print(f"   (filtered by: {', '.join(filters_applied)})")

    except Exception as e:
        print(f"‚ùå Error exporting data: {e}")

def handle_extract_urls_command(args):
    """Extract all URLs from an HTML file"""
    from utils.url_extractor import extract_urls_from_html

    try:
        urls = extract_urls_from_html(args.file, args.output)

        # If no output file specified, print to stdout
        if not args.output:
            print('\n'.join(urls))
        else:
            print(f"‚úÖ Extracted {len(urls)} URLs to {args.output}")

    except FileNotFoundError as e:
        print(f"‚ùå {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

def handle_projects_command(args):
    """Handle project management commands"""
    pm = ProjectManager()
    
    if args.projects_command == 'create':
        spiders = [s.strip() for s in args.spiders.split(',')]
        pm.create_project(args.name, spiders)
    elif args.projects_command == 'list':
        projects = pm.list_projects()
        if projects:
            print("üìã Available projects:")
            for project in projects:
                print(f"  ‚Ä¢ {project}")
        else:
            print("No projects found. Create one with: ./scrapai projects create --name <name> --spiders <spider1,spider2>")
    elif args.projects_command == 'status':
        status = pm.get_project_status(args.project)
        if 'error' in status:
            print(f"‚ùå {status['error']}")
        else:
            print(f"üìä Project: {status['name']}")
            print(f"üï∑Ô∏è  Spiders: {', '.join(status['spiders'])}")
            print(f"üìÑ Output files: {status['output_files']}")
            print(f"üìù Log files: {status['log_files']}")
            print(f"üìÅ Path: {status['path']}")
    elif args.projects_command == 'delete':
        confirm = input(f"Are you sure you want to delete project '{args.project}'? (y/N): ")
        if confirm.lower() == 'y':
            pm.delete_project(args.project)
        else:
            print("‚ùå Delete cancelled")

def run_spider(project_name, spider_name, output_file=None, limit=None, timeout=None):
    """Run a Scrapy spider (DB or File-based)"""
    # Try to find spider in DB first
    from core.db import get_db
    from core.models import Spider

    db = next(get_db())
    db_spider = db.query(Spider).filter(Spider.name == spider_name).first()

    if db_spider:
        print(f"üöÄ Running DB spider: {spider_name}")
        import sys
        from datetime import datetime

        # Check if spider uses Cloudflare bypass
        cf_enabled = False
        if db_spider.settings:
            for setting in db_spider.settings:
                if setting.key == "CLOUDFLARE_ENABLED" and str(setting.value).lower() in ["true", "1"]:
                    cf_enabled = True
                    break

        cmd = [sys.executable, '-m', 'scrapy', 'crawl', 'database_spider', '-a', f'spider_name={spider_name}']

        # Smart storage logic based on --limit flag
        if limit:
            # Testing mode: Save to database for verification with `show` command
            print(f"üß™ Test mode: Saving to database (limit: {limit} items)")
            print(f"   Use './scrapai show {spider_name}' to verify results")
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
            # Don't include HTML in test mode (saves DB storage)
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=False'])
        else:
            # Production mode: Export to files, disable database to save costs
            print(f"üìÅ Production mode: Exporting to files (database disabled)")

            # Disable DatabasePipeline to avoid DB costs
            cmd.extend(['-s', 'ITEM_PIPELINES={"pipelines.ScrapaiPipeline": 300}'])

            # Include HTML in JSONL output for full scraping
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=True'])

            # Add default output file if not specified
            if not output_file:
                import os
                now = datetime.now()
                date_folder = now.strftime('%Y-%m-%d')
                output_dir = f'data/{spider_name}/{date_folder}'
                os.makedirs(output_dir, exist_ok=True)
                timestamp = now.strftime('%H%M%S')
                output_file = f'{output_dir}/crawl_{timestamp}.jsonl'

            cmd.extend(['-o', output_file])
            print(f"   Output: {output_file} (includes HTML)")

        # Add explicit output file if provided
        if output_file and limit:
            cmd.extend(['-o', output_file])
            print(f"   Also saving to: {output_file}")

        # Add graceful timeout if specified (CLOSESPIDER_TIMEOUT)
        if timeout:
            cmd.extend(['-s', f'CLOSESPIDER_TIMEOUT={timeout}'])
            hours = timeout / 3600
            print(f"‚è±Ô∏è  Max runtime: {hours:.1f} hours (graceful stop)")

        # Auto-detect if xvfb is needed for Cloudflare bypass
        if cf_enabled:
            from utils.display_helper import needs_xvfb, has_xvfb

            if needs_xvfb():
                if has_xvfb():
                    print("üñ•Ô∏è  Headless environment detected - using xvfb for Cloudflare bypass")
                    cmd = ['xvfb-run', '-a'] + cmd
                else:
                    print("‚ö†Ô∏è  WARNING: Cloudflare bypass enabled but no display available and xvfb not installed")
                    print("   Install xvfb: sudo apt-get install xvfb")
                    print("   Continuing anyway - browser may fail to start...")
            else:
                print("üñ•Ô∏è  Display available - using native browser for Cloudflare bypass")

        subprocess.run(cmd)
        return

    # Fallback to legacy file-based project system
    if not project_name:
        print(f"‚ùå Spider '{spider_name}' not found in DB. To run legacy file-based spider, please provide --project.")
        return

    config_loader = ConfigLoader()
    
    try:
        # Validate project and spider
        if not config_loader.validate_spider_for_project(project_name, spider_name):
            print(f"‚ùå Spider '{spider_name}' not found in DB and not configured for project '{project_name}'")
            return
        
        # Get project settings
        settings = config_loader.get_spider_settings(project_name, spider_name)
        
        # Build scrapy command
        import sys
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', spider_name]
        
        # Add settings (only simple ones that can be passed via command line)
        for key, value in settings.items():
            if key not in ['FEEDS', 'LOG_FILE', 'ITEM_PIPELINES']:  # These are handled differently
                # Only add simple settings that can be passed via command line
                if isinstance(value, (str, int, float, bool)):
                    cmd.extend(['-s', f'{key}={value}'])
        
        # Handle output
        if output_file:
            cmd.extend(['-o', output_file])
        else:
            # Create output directory and add default output
            import os
            output_dir = f'projects/{project_name}/outputs/{spider_name}'
            os.makedirs(output_dir, exist_ok=True)
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'{output_dir}/{timestamp}.json'
            cmd.extend(['-o', output_file])
        
        # Handle log file
        if 'LOG_FILE' in settings:
            cmd.extend(['-s', f'LOG_FILE={settings["LOG_FILE"]}'])
        
        if limit:
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
        
        print(f"üöÄ Running spider: {spider_name} (project: {project_name})")
        subprocess.run(cmd)
        
    except Exception as e:
        print(f"‚ùå Error running spider: {e}")

def run_all_spiders(project_name, limit=None):
    """Run all spiders configured for a project"""
    config_loader = ConfigLoader()
    
    try:
        spiders = config_loader.get_project_spiders(project_name)
        if not spiders:
            print(f"‚ùå No spiders configured for project '{project_name}'")
            return
        
        print(f"üöÄ Running all spiders for project: {project_name}")
        print(f"üï∑Ô∏è  Spiders: {', '.join(spiders)}")
        
        for spider in spiders:
            print(f"\n{'='*50}")
            print(f"Running: {spider}")
            print(f"{'='*50}")
            run_spider(project_name, spider, None, limit)
            
    except Exception as e:
        print(f"‚ùå Error running spiders: {e}")

def test_spider(project_name, spider_name, limit=5):
    """Test a spider with limited items"""
    print(f"üß™ Testing spider: {spider_name} (project: {project_name}, limit: {limit})")
    run_spider(project_name, spider_name, None, limit)

def list_spiders(project_name=None):
    """List available spiders"""
    if project_name:
        # List spiders for specific project
        config_loader = ConfigLoader()
        try:
            spiders = config_loader.get_project_spiders(project_name)
            print(f"üìã Spiders configured for project '{project_name}':")
            for spider in spiders:
                print(f"  ‚Ä¢ {spider}")
        except Exception as e:
            print(f"‚ùå Error: {e}")
    else:
        # List all available spiders
        print("üìã Available spiders:")
        spiders_dir = Path("spiders")
        if spiders_dir.exists():
            for spider_file in spiders_dir.glob("*.py"):
                if spider_file.name != "__init__.py" and spider_file.name != "base_spider.py":
                    spider_name = spider_file.stem
                    print(f"  ‚Ä¢ {spider_name}")
        else:
            print("  No spiders found. Ask Claude Code to create some!")

def show_project_status(project_name):
    """Show project status"""
    pm = ProjectManager()
    status = pm.get_project_status(project_name)
    
    if 'error' in status:
        print(f"‚ùå {status['error']}")
    else:
        print(f"üìä Project: {status['name']}")
        print(f"üï∑Ô∏è  Spiders: {', '.join(status['spiders'])}")
        print(f"üìÑ Output files: {status['output_files']}")
        print(f"üìù Log files: {status['log_files']}")
        print(f"üìÅ Path: {status['path']}")

def show_project_logs(project_name, spider_name=None):
    """Show project logs"""
    pm = ProjectManager()
    
    if not pm.project_exists(project_name):
        print(f"‚ùå Project '{project_name}' not found")
        return
    
    logs_dir = pm.get_project_path(project_name) / "logs"
    
    if not logs_dir.exists():
        print(f"üìù No logs found for project '{project_name}'")
        return
    
    if spider_name:
        # Show specific spider logs
        log_file = logs_dir / f"{spider_name}.log"
        if log_file.exists():
            print(f"üìù Logs for {spider_name} (project: {project_name}):")
            print("-" * 50)
            with open(log_file, 'r') as f:
                print(f.read())
        else:
            print(f"‚ùå No logs found for spider '{spider_name}' in project '{project_name}'")
    else:
        # Show all log files
        log_files = list(logs_dir.glob("*.log"))
        if log_files:
            print(f"üìù Log files for project '{project_name}':")
            for log_file in log_files:
                print(f"  ‚Ä¢ {log_file.name}")
        else:
            print(f"üìù No log files found for project '{project_name}'")

if __name__ == '__main__':
    main()