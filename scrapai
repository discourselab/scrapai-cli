#!/usr/bin/env python3
"""
scrapai - Simple Scrapy CLI for running Claude Code generated spiders with project support
"""

import argparse
import subprocess
import sys
import os
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Only import dependencies if not running setup command or if dependencies are available
try:
    from core.project_manager import ProjectManager
    from core.config_loader import ConfigLoader
except ImportError:
    if len(sys.argv) <= 1 or (sys.argv[1] not in ['setup', 'verify', '--help', '-h']):
        # If it's not setup/verify/help command and imports fail, show error
        print("âŒ Dependencies not installed. Run './scrapai setup' first.")
        sys.exit(1)
    # For setup/verify/help command, continue without imports
    ProjectManager = None
    ConfigLoader = None

def check_venv():
    """Check if we're in a virtual environment, warn if not"""
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        venv_path = Path('.venv')
        if venv_path.exists():
            print("âš ï¸  Virtual environment detected but not activated!")
            print("Run: source .venv/bin/activate")
            print("Then try again.\n")
        else:
            print("âš ï¸  No virtual environment found!")
            print("Run: uv venv && source .venv/bin/activate && uv pip install -r requirements.txt")
            print("Then try again.\n")
        return False
    return True

def main():
    # Skip venv check for setup and verify commands
    if len(sys.argv) > 1 and sys.argv[1] in ['setup', 'verify', '--help', '-h']:
        pass  # Allow setup/verify/help commands to run without venv
    else:
        if not check_venv():
            sys.exit(1)
    
    parser = argparse.ArgumentParser(description='Run Scrapy spiders with project support')
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # projects command
    projects_parser = subparsers.add_parser('projects', help='Project management')
    projects_subparsers = projects_parser.add_subparsers(dest='projects_command', help='Project commands')
    
    # projects create
    create_parser = projects_subparsers.add_parser('create', help='Create a new project')
    create_parser.add_argument('--name', required=True, help='Project name')
    create_parser.add_argument('--spiders', required=True, help='Comma-separated spider names')
    
    # projects list
    projects_subparsers.add_parser('list', help='List all projects')
    
    # projects status
    status_parser = projects_subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # projects delete
    delete_parser = projects_subparsers.add_parser('delete', help='Delete a project')
    delete_parser.add_argument('--project', required=True, help='Project name')
    
    # crawl command
    crawl_parser = subparsers.add_parser('crawl', help='Run a spider')
    crawl_parser.add_argument('--project', help='Project name (optional for DB spiders)')
    crawl_parser.add_argument('spider', help='Spider name to run')
    crawl_parser.add_argument('--output', '-o', help='Output file path (optional, uses project config)')
    crawl_parser.add_argument('--limit', '-l', type=int, help='Limit number of items')
    
    # crawl-all command
    crawl_all_parser = subparsers.add_parser('crawl-all', help='Run all spiders in a project')
    crawl_all_parser.add_argument('--project', required=True, help='Project name')
    crawl_all_parser.add_argument('--limit', '-l', type=int, help='Limit number of items per spider')
    
    # list command
    list_parser = subparsers.add_parser('list', help='List available spiders')
    list_parser.add_argument('--project', help='Project name (optional)')
    
    # test command
    test_parser = subparsers.add_parser('test', help='Test a spider with limited items')
    test_parser.add_argument('--project', required=True, help='Project name')
    test_parser.add_argument('spider', help='Spider name to test')
    test_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of items to test')
    
    # status command
    status_parser = subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # spiders command
    spiders_parser = subparsers.add_parser('spiders', help='Spider management')
    spiders_subparsers = spiders_parser.add_subparsers(dest='spiders_command', help='Spider commands')
    
    # spiders list
    spiders_subparsers.add_parser('list', help='List all spiders in DB')
    
    # spiders import
    import_parser = spiders_subparsers.add_parser('import', help='Import spider from JSON')
    import_parser.add_argument('file', help='JSON file containing spider definition (use "-" to read from stdin)')
    
    # spiders delete
    delete_parser = spiders_subparsers.add_parser('delete', help='Delete a spider')
    delete_parser.add_argument('name', help='Spider name')
    delete_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # db command for database operations
    db_parser = subparsers.add_parser('db', help='Database management')
    db_subparsers = db_parser.add_subparsers(dest='db_command', help='Database commands')
    
    # db migrate
    db_subparsers.add_parser('migrate', help='Run database migrations')
    
    # db current
    db_subparsers.add_parser('current', help='Show current migration revision')

    # show command
    show_parser = subparsers.add_parser('show', help='Show scraped articles from database')
    show_parser.add_argument('spider_name', help='Spider name to show articles from')
    show_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of articles to show (default: 5)')
    show_parser.add_argument('--url', help='Show only articles containing this URL pattern')
    show_parser.add_argument('--text', '-t', help='Show only articles containing this text in title or content')
    show_parser.add_argument('--title', help='Show only articles containing this text in title')

    # setup command
    subparsers.add_parser('setup', help='Setup virtual environment and database')

    # verify command
    subparsers.add_parser('verify', help='Verify environment setup (no installations)')

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Handle commands
    if args.command == 'setup':
        handle_setup_command()
    elif args.command == 'verify':
        handle_verify_command()
    elif args.command == 'projects':
        handle_projects_command(args)
    elif args.command == 'spiders':
        handle_spiders_command(args)
    elif args.command == 'db':
        handle_db_command(args)
    elif args.command == 'show':
        handle_show_command(args)
    elif args.command == 'crawl':
        run_spider(args.project, args.spider, args.output, args.limit)
    elif args.command == 'crawl-all':
        run_all_spiders(args.project, args.limit)
    elif args.command == 'list':
        list_spiders(args.project)
    elif args.command == 'test':
        test_spider(args.project, args.spider, args.limit)
    elif args.command == 'status':
        show_project_status(args.project)
    elif args.command == 'logs':
        show_project_logs(args.project, args.spider)

def handle_setup_command():
    """Handle setup command - automated virtual environment and database setup"""
    import subprocess
    import sys
    from pathlib import Path
    
    print("ğŸš€ Setting up ScrapAI environment...")
    
    # Check if virtual environment exists
    venv_path = Path('.venv')
    if not venv_path.exists():
        print("ğŸ“¦ Creating virtual environment...")
        try:
            subprocess.run([sys.executable, '-m', 'venv', '.venv'], check=True)
            print("âœ… Virtual environment created")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to create virtual environment: {e}")
            sys.exit(1)
    else:
        print("âœ… Virtual environment already exists")
    
    # Check if we're in a virtual environment
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âš ï¸  Virtual environment not activated")
        print("ğŸ”„ Run: source .venv/bin/activate")
        print("ğŸ”„ Then run: ./scrapai setup")
        return
    
    # Install requirements if needed
    requirements_path = Path('requirements.txt')
    if requirements_path.exists():
        print("ğŸ“‹ Installing requirements...")
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)
            print("âœ… Requirements installed")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install requirements: {e}")
            sys.exit(1)
    else:
        print("âš ï¸  requirements.txt not found")
    
    # Check database setup
    print("ğŸ—„ï¸  Checking database setup...")
    try:
        # Try to check current migration state
        result = subprocess.run(['./scrapai', 'db', 'current'], capture_output=True, text=True)
        if result.returncode == 0 and 'head' in result.stdout:
            print("âœ… Database already initialized")
        else:
            print("ğŸ”§ Initializing database...")
            subprocess.run(['python', './init_db.py'], check=True)
            print("âœ… Database initialized")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Database setup failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Setup error: {e}")
        sys.exit(1)
    
    print("ğŸ‰ ScrapAI setup complete!")
    print("ğŸ“ You can now:")
    print("   â€¢ List spiders: ./scrapai spiders list")
    print("   â€¢ Import spiders: ./scrapai spiders import <file>")
    print("   â€¢ Run crawls: ./scrapai crawl <spider_name>")

def handle_verify_command():
    """Verify environment setup without installing anything"""
    import subprocess
    import sys
    from pathlib import Path

    print("ğŸ” Verifying ScrapAI environment...\n")

    all_good = True

    # Check if virtual environment exists
    venv_path = Path('.venv')
    if not venv_path.exists():
        print("âŒ Virtual environment not found")
        print("   Run: ./scrapai setup")
        all_good = False
    else:
        print("âœ… Virtual environment exists")

    # Check if we're in a virtual environment
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âŒ Virtual environment not activated")
        print("   Run: source .venv/bin/activate")
        all_good = False
    else:
        print("âœ… Virtual environment activated")

        # Check if key dependencies are installed (only if venv is activated)
        try:
            import scrapy
            import sqlalchemy
            import alembic
            print("âœ… Core dependencies installed")
        except ImportError as e:
            print(f"âŒ Missing dependencies: {e}")
            print("   Run: source .venv/bin/activate && ./scrapai setup")
            all_good = False

    # Check database setup (only if venv is activated and dependencies exist)
    if all_good:
        try:
            result = subprocess.run(
                [sys.executable, '-m', 'alembic', 'current'],
                capture_output=True,
                text=True,
                cwd=os.path.dirname(os.path.abspath(__file__))
            )
            if result.returncode == 0:
                if 'head' in result.stdout or result.stdout.strip():
                    print("âœ… Database initialized")
                else:
                    print("âŒ Database not initialized")
                    print("   Run: source .venv/bin/activate && ./scrapai setup")
                    all_good = False
            else:
                print("âŒ Unable to check database status")
                print("   Run: source .venv/bin/activate && ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"âŒ Error checking database: {e}")
            print("   Run: source .venv/bin/activate && ./scrapai setup")
            all_good = False

    print()
    if all_good:
        print("ğŸ‰ Environment is ready!")
        print("ğŸ“ You can now:")
        print("   â€¢ List spiders: ./scrapai spiders list")
        print("   â€¢ Import spiders: ./scrapai spiders import <file>")
        print("   â€¢ Run crawls: ./scrapai crawl <spider_name>")
    else:
        print("âš ï¸  Environment setup incomplete")
        print("   Run: ./scrapai setup")

def handle_spiders_command(args):
    """Handle spider management commands"""
    from core.db import get_db
    from core.models import Spider, SpiderRule, SpiderSetting
    import json
    
    db = next(get_db())
    
    if args.spiders_command == 'list':
        spiders = db.query(Spider).all()
        if spiders:
            print("ğŸ“‹ Available Spiders (DB):")
            for s in spiders:
                created = s.created_at.strftime('%Y-%m-%d %H:%M') if s.created_at else 'Unknown'
                updated = s.updated_at.strftime('%Y-%m-%d %H:%M') if hasattr(s, 'updated_at') and s.updated_at else created
                print(f"  â€¢ {s.name} (Active: {s.active}) - Created: {created}, Updated: {updated}")
        else:
            print("No spiders found in database.")
            
    elif args.spiders_command == 'import':
        try:
            if args.file == '-':
                # Read from stdin
                import sys
                data = json.load(sys.stdin)
            else:
                # Read from file
                with open(args.file, 'r') as f:
                    data = json.load(f)
            
            # Check if spider exists
            existing = db.query(Spider).filter(Spider.name == data['name']).first()
            if existing:
                print(f"âš ï¸  Spider '{data['name']}' already exists. Updating...")
                # Simple update: delete and recreate (or just update fields)
                # For now, let's just update the main fields and replace rules
                existing.allowed_domains = data['allowed_domains']
                existing.start_urls = data['start_urls']
                
                # Clear old rules and settings
                db.query(SpiderRule).filter(SpiderRule.spider_id == existing.id).delete()
                db.query(SpiderSetting).filter(SpiderSetting.spider_id == existing.id).delete()
                spider = existing
            else:
                spider = Spider(
                    name=data['name'],
                    allowed_domains=data['allowed_domains'],
                    start_urls=data['start_urls']
                )
                db.add(spider)
                db.flush() # Get ID
            
            # Add Rules
            for r_data in data.get('rules', []):
                rule = SpiderRule(
                    spider_id=spider.id,
                    allow_patterns=r_data.get('allow'),
                    deny_patterns=r_data.get('deny'),
                    restrict_xpaths=r_data.get('restrict_xpaths'),
                    restrict_css=r_data.get('restrict_css'),
                    callback=r_data.get('callback'),
                    follow=r_data.get('follow', True),
                    priority=r_data.get('priority', 0)
                )
                db.add(rule)
            
            # Add Settings
            for k, v in data.get('settings', {}).items():
                setting = SpiderSetting(
                    spider_id=spider.id,
                    key=k,
                    value=str(v),
                    type=type(v).__name__
                )
                db.add(setting)
            
            db.commit()
            print(f"âœ… Spider '{spider.name}' imported successfully!")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ Error importing spider: {e}")
            
    elif args.spiders_command == 'delete':
        spider = db.query(Spider).filter(Spider.name == args.name).first()
        if spider:
            if not args.force:
                confirm = input(f"Are you sure you want to delete spider '{args.name}'? (y/N): ")
                if confirm.lower() != 'y':
                    print("âŒ Delete cancelled")
                    return
            
            db.delete(spider)
            db.commit()
            print(f"ğŸ—‘ï¸  Spider '{args.name}' deleted!")
        else:
            print(f"âŒ Spider '{args.name}' not found.")

def handle_db_command(args):
    """Handle database management commands"""
    import subprocess
    import sys
    import os
    
    if args.db_command == 'migrate':
        print("ğŸ”„ Running database migrations...")
        try:
            result = subprocess.run([
                sys.executable, '-m', 'alembic', 'upgrade', 'head'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
            if result.returncode == 0:
                print("âœ… Migrations completed successfully!")
            else:
                print("âŒ Migration failed!")
        except Exception as e:
            print(f"âŒ Error running migrations: {e}")
            
    elif args.db_command == 'current':
        try:
            subprocess.run([
                sys.executable, '-m', 'alembic', 'current'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
        except Exception as e:
            print(f"âŒ Error checking current revision: {e}")

def handle_show_command(args):
    """Show scraped articles from database"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from datetime import datetime
    
    db = next(get_db())
    
    # Find spider by name
    spider = db.query(Spider).filter(Spider.name == args.spider_name).first()
    if not spider:
        print(f"âŒ Spider '{args.spider_name}' not found in database.")
        return
    
    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)
    
    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")
    
    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")
    
    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")
    
    # Get items ordered by most recent first
    items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()
    
    if not items:
        print(f"ğŸ“­ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return
    
    print(f"ğŸ“° Showing {len(items)} articles from '{args.spider_name}':")
    if filters_applied:
        print(f"   (filtered by: {', '.join(filters_applied)})")
    print()
    
    for i, item in enumerate(items, 1):
        # Format date nicely
        scraped_date = item.scraped_at.strftime('%Y-%m-%d %H:%M') if item.scraped_at else 'Unknown'
        pub_date = item.published_date.strftime('%Y-%m-%d') if item.published_date else 'Unknown'
        
        print(f"ğŸ”¸ [{i}] {item.title or 'No Title'}")
        print(f"   ğŸ“… Published: {pub_date} | Scraped: {scraped_date}")
        print(f"   ğŸ”— {item.url}")
        if item.author:
            print(f"   âœï¸  {item.author}")
        
        # Show content preview (first 150 chars)
        if item.content:
            content_preview = item.content[:150].replace('\n', ' ').strip()
            if len(item.content) > 150:
                content_preview += "..."
            print(f"   ğŸ“ {content_preview}")
        print()

def handle_projects_command(args):
    """Handle project management commands"""
    pm = ProjectManager()
    
    if args.projects_command == 'create':
        spiders = [s.strip() for s in args.spiders.split(',')]
        pm.create_project(args.name, spiders)
    elif args.projects_command == 'list':
        projects = pm.list_projects()
        if projects:
            print("ğŸ“‹ Available projects:")
            for project in projects:
                print(f"  â€¢ {project}")
        else:
            print("No projects found. Create one with: ./scrapai projects create --name <name> --spiders <spider1,spider2>")
    elif args.projects_command == 'status':
        status = pm.get_project_status(args.project)
        if 'error' in status:
            print(f"âŒ {status['error']}")
        else:
            print(f"ğŸ“Š Project: {status['name']}")
            print(f"ğŸ•·ï¸  Spiders: {', '.join(status['spiders'])}")
            print(f"ğŸ“„ Output files: {status['output_files']}")
            print(f"ğŸ“ Log files: {status['log_files']}")
            print(f"ğŸ“ Path: {status['path']}")
    elif args.projects_command == 'delete':
        confirm = input(f"Are you sure you want to delete project '{args.project}'? (y/N): ")
        if confirm.lower() == 'y':
            pm.delete_project(args.project)
        else:
            print("âŒ Delete cancelled")

def run_spider(project_name, spider_name, output_file=None, limit=None):
    """Run a Scrapy spider (DB or File-based)"""
    # Try to find spider in DB first
    from core.db import get_db
    from core.models import Spider
    
    db = next(get_db())
    db_spider = db.query(Spider).filter(Spider.name == spider_name).first()
    
    if db_spider:
        print(f"ğŸš€ Running DB spider: {spider_name}")
        import sys
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', 'database_spider', '-a', f'spider_name={spider_name}']
        
        # Add settings from DB (handled inside DatabaseSpider, but we can pass overrides here if needed)
        # Note: DatabaseSpider loads its own settings, but we might want to pass CLI overrides
        
        # For database spiders, data is saved via DatabasePipeline
        # Only add JSON output if explicitly requested
        if output_file:
            cmd.extend(['-o', output_file])
            print(f"ğŸ“„ Output will also be saved to: {output_file}")
        
        print(f"ğŸ’¾ Data will be saved to database via DatabasePipeline")
            
        if limit:
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
            
        subprocess.run(cmd)
        return

    # Fallback to legacy file-based project system
    if not project_name:
        print(f"âŒ Spider '{spider_name}' not found in DB. To run legacy file-based spider, please provide --project.")
        return

    config_loader = ConfigLoader()
    
    try:
        # Validate project and spider
        if not config_loader.validate_spider_for_project(project_name, spider_name):
            print(f"âŒ Spider '{spider_name}' not found in DB and not configured for project '{project_name}'")
            return
        
        # Get project settings
        settings = config_loader.get_spider_settings(project_name, spider_name)
        
        # Build scrapy command
        import sys
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', spider_name]
        
        # Add settings (only simple ones that can be passed via command line)
        for key, value in settings.items():
            if key not in ['FEEDS', 'LOG_FILE', 'ITEM_PIPELINES']:  # These are handled differently
                # Only add simple settings that can be passed via command line
                if isinstance(value, (str, int, float, bool)):
                    cmd.extend(['-s', f'{key}={value}'])
        
        # Handle output
        if output_file:
            cmd.extend(['-o', output_file])
        else:
            # Create output directory and add default output
            import os
            output_dir = f'projects/{project_name}/outputs/{spider_name}'
            os.makedirs(output_dir, exist_ok=True)
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'{output_dir}/{timestamp}.json'
            cmd.extend(['-o', output_file])
        
        # Handle log file
        if 'LOG_FILE' in settings:
            cmd.extend(['-s', f'LOG_FILE={settings["LOG_FILE"]}'])
        
        if limit:
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
        
        print(f"ğŸš€ Running spider: {spider_name} (project: {project_name})")
        subprocess.run(cmd)
        
    except Exception as e:
        print(f"âŒ Error running spider: {e}")

def run_all_spiders(project_name, limit=None):
    """Run all spiders configured for a project"""
    config_loader = ConfigLoader()
    
    try:
        spiders = config_loader.get_project_spiders(project_name)
        if not spiders:
            print(f"âŒ No spiders configured for project '{project_name}'")
            return
        
        print(f"ğŸš€ Running all spiders for project: {project_name}")
        print(f"ğŸ•·ï¸  Spiders: {', '.join(spiders)}")
        
        for spider in spiders:
            print(f"\n{'='*50}")
            print(f"Running: {spider}")
            print(f"{'='*50}")
            run_spider(project_name, spider, None, limit)
            
    except Exception as e:
        print(f"âŒ Error running spiders: {e}")

def test_spider(project_name, spider_name, limit=5):
    """Test a spider with limited items"""
    print(f"ğŸ§ª Testing spider: {spider_name} (project: {project_name}, limit: {limit})")
    run_spider(project_name, spider_name, None, limit)

def list_spiders(project_name=None):
    """List available spiders"""
    if project_name:
        # List spiders for specific project
        config_loader = ConfigLoader()
        try:
            spiders = config_loader.get_project_spiders(project_name)
            print(f"ğŸ“‹ Spiders configured for project '{project_name}':")
            for spider in spiders:
                print(f"  â€¢ {spider}")
        except Exception as e:
            print(f"âŒ Error: {e}")
    else:
        # List all available spiders
        print("ğŸ“‹ Available spiders:")
        spiders_dir = Path("spiders")
        if spiders_dir.exists():
            for spider_file in spiders_dir.glob("*.py"):
                if spider_file.name != "__init__.py" and spider_file.name != "base_spider.py":
                    spider_name = spider_file.stem
                    print(f"  â€¢ {spider_name}")
        else:
            print("  No spiders found. Ask Claude Code to create some!")

def show_project_status(project_name):
    """Show project status"""
    pm = ProjectManager()
    status = pm.get_project_status(project_name)
    
    if 'error' in status:
        print(f"âŒ {status['error']}")
    else:
        print(f"ğŸ“Š Project: {status['name']}")
        print(f"ğŸ•·ï¸  Spiders: {', '.join(status['spiders'])}")
        print(f"ğŸ“„ Output files: {status['output_files']}")
        print(f"ğŸ“ Log files: {status['log_files']}")
        print(f"ğŸ“ Path: {status['path']}")

def show_project_logs(project_name, spider_name=None):
    """Show project logs"""
    pm = ProjectManager()
    
    if not pm.project_exists(project_name):
        print(f"âŒ Project '{project_name}' not found")
        return
    
    logs_dir = pm.get_project_path(project_name) / "logs"
    
    if not logs_dir.exists():
        print(f"ğŸ“ No logs found for project '{project_name}'")
        return
    
    if spider_name:
        # Show specific spider logs
        log_file = logs_dir / f"{spider_name}.log"
        if log_file.exists():
            print(f"ğŸ“ Logs for {spider_name} (project: {project_name}):")
            print("-" * 50)
            with open(log_file, 'r') as f:
                print(f.read())
        else:
            print(f"âŒ No logs found for spider '{spider_name}' in project '{project_name}'")
    else:
        # Show all log files
        log_files = list(logs_dir.glob("*.log"))
        if log_files:
            print(f"ğŸ“ Log files for project '{project_name}':")
            for log_file in log_files:
                print(f"  â€¢ {log_file.name}")
        else:
            print(f"ğŸ“ No log files found for project '{project_name}'")

if __name__ == '__main__':
    main()