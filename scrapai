#!/usr/bin/env python3
"""
scrapai - Simple Scrapy CLI for running Claude Code generated spiders with project support
"""

import argparse
import subprocess
import sys
import os
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Only import dependencies if not running setup command or if dependencies are available
try:
    from core.project_manager import ProjectManager
    from core.config_loader import ConfigLoader
except ImportError:
    if len(sys.argv) <= 1 or (sys.argv[1] not in ['setup', 'verify', '--help', '-h']):
        # If it's not setup/verify/help command and imports fail, show error
        print("âŒ Dependencies not installed. Run './scrapai setup' first.")
        sys.exit(1)
    # For setup/verify/help command, continue without imports
    ProjectManager = None
    ConfigLoader = None

def check_venv():
    """Check if we're in a virtual environment, warn if not"""
    # Skip venv check if running in Docker/containerized environment
    if os.getenv('SKIP_VENV_CHECK') or os.getenv('AIRFLOW_HOME'):
        return True

    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        venv_path = Path('.venv')
        if venv_path.exists():
            print("âš ï¸  Virtual environment detected but not activated!")
            print("Run: source .venv/bin/activate")
            print("Then try again.\n")
        else:
            print("âš ï¸  No virtual environment found!")
            print("Run: uv venv && source .venv/bin/activate && uv pip install -r requirements.txt")
            print("Then try again.\n")
        return False
    return True

def main():
    # Skip venv check for setup and verify commands
    if len(sys.argv) > 1 and sys.argv[1] in ['setup', 'verify', '--help', '-h']:
        pass  # Allow setup/verify/help commands to run without venv
    else:
        if not check_venv():
            sys.exit(1)
    
    parser = argparse.ArgumentParser(description='Run Scrapy spiders with project support')
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # projects command
    projects_parser = subparsers.add_parser('projects', help='Project management')
    projects_subparsers = projects_parser.add_subparsers(dest='projects_command', help='Project commands')
    
    # projects create
    create_parser = projects_subparsers.add_parser('create', help='Create a new project')
    create_parser.add_argument('--name', required=True, help='Project name')
    create_parser.add_argument('--spiders', required=True, help='Comma-separated spider names')
    
    # projects list
    projects_subparsers.add_parser('list', help='List all projects')
    
    # projects status
    status_parser = projects_subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # projects delete
    delete_parser = projects_subparsers.add_parser('delete', help='Delete a project')
    delete_parser.add_argument('--project', required=True, help='Project name')
    
    # crawl command
    crawl_parser = subparsers.add_parser('crawl', help='Run a spider')
    crawl_parser.add_argument('--project', help='Project name (optional for DB spiders)')
    crawl_parser.add_argument('spider', help='Spider name to run')
    crawl_parser.add_argument('--output', '-o', help='Output file path (optional, uses project config)')
    crawl_parser.add_argument('--limit', '-l', type=int, help='Limit number of items')
    
    # crawl-all command
    crawl_all_parser = subparsers.add_parser('crawl-all', help='Run all spiders in a project')
    crawl_all_parser.add_argument('--project', required=True, help='Project name')
    crawl_all_parser.add_argument('--limit', '-l', type=int, help='Limit number of items per spider')
    
    # list command
    list_parser = subparsers.add_parser('list', help='List available spiders')
    list_parser.add_argument('--project', help='Project name (optional)')
    
    # test command
    test_parser = subparsers.add_parser('test', help='Test a spider with limited items')
    test_parser.add_argument('--project', required=True, help='Project name')
    test_parser.add_argument('spider', help='Spider name to test')
    test_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of items to test')
    
    # status command
    status_parser = subparsers.add_parser('status', help='Show project status')
    status_parser.add_argument('--project', required=True, help='Project name')
    
    # spiders command
    spiders_parser = subparsers.add_parser('spiders', help='Spider management')
    spiders_subparsers = spiders_parser.add_subparsers(dest='spiders_command', help='Spider commands')
    
    # spiders list
    spiders_subparsers.add_parser('list', help='List all spiders in DB')
    
    # spiders import
    import_parser = spiders_subparsers.add_parser('import', help='Import spider from JSON')
    import_parser.add_argument('file', help='JSON file containing spider definition (use "-" to read from stdin)')
    
    # spiders delete
    delete_parser = spiders_subparsers.add_parser('delete', help='Delete a spider')
    delete_parser.add_argument('name', help='Spider name')
    delete_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # queue command for queue management
    queue_parser = subparsers.add_parser('queue', help='Queue management')
    queue_subparsers = queue_parser.add_subparsers(dest='queue_command', help='Queue commands')

    # queue add
    add_parser = queue_subparsers.add_parser('add', help='Add website to queue')
    add_parser.add_argument('url', help='Website URL to add to queue')
    add_parser.add_argument('-m', '--message', dest='custom_instruction', help='Custom instruction for processing')
    add_parser.add_argument('--priority', type=int, default=5, help='Priority (higher = sooner, default: 5)')
    add_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue list
    list_parser = queue_subparsers.add_parser('list', help='List queue items (excludes failed/completed by default)')
    list_parser.add_argument('--project', default='default', help='Project name (default: default)')
    list_parser.add_argument('--status', help='Filter by status (pending, processing, completed, failed)')
    list_parser.add_argument('--limit', type=int, default=5, help='Limit number of items to show (default: 5)')
    list_parser.add_argument('--all', action='store_true', help='Show all items including failed and completed')

    # queue next
    next_parser = queue_subparsers.add_parser('next', help='Get next item from queue (atomic claim)')
    next_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue complete
    complete_parser = queue_subparsers.add_parser('complete', help='Mark item as completed')
    complete_parser.add_argument('id', type=int, help='Queue item ID')

    # queue fail
    fail_parser = queue_subparsers.add_parser('fail', help='Mark item as failed')
    fail_parser.add_argument('id', type=int, help='Queue item ID')
    fail_parser.add_argument('-m', '--message', dest='error_message', help='Error message')

    # queue retry
    retry_parser = queue_subparsers.add_parser('retry', help='Retry a failed item')
    retry_parser.add_argument('id', type=int, help='Queue item ID')

    # queue remove
    remove_parser = queue_subparsers.add_parser('remove', help='Remove item from queue')
    remove_parser.add_argument('id', type=int, help='Queue item ID')

    # queue cleanup
    cleanup_parser = queue_subparsers.add_parser('cleanup', help='Bulk cleanup queue items')
    cleanup_parser.add_argument('--completed', action='store_true', help='Remove all completed items')
    cleanup_parser.add_argument('--failed', action='store_true', help='Remove all failed items')
    cleanup_parser.add_argument('--all', action='store_true', help='Remove all completed and failed items')
    cleanup_parser.add_argument('--project', default='default', help='Project name (default: default)')
    cleanup_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # db command for database operations
    db_parser = subparsers.add_parser('db', help='Database management')
    db_subparsers = db_parser.add_subparsers(dest='db_command', help='Database commands')

    # db migrate
    db_subparsers.add_parser('migrate', help='Run database migrations')

    # db current
    db_subparsers.add_parser('current', help='Show current migration revision')

    # show command
    show_parser = subparsers.add_parser('show', help='Show scraped articles from database')
    show_parser.add_argument('spider_name', help='Spider name to show articles from')
    show_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of articles to show (default: 5)')
    show_parser.add_argument('--url', help='Show only articles containing this URL pattern')
    show_parser.add_argument('--text', '-t', help='Show only articles containing this text in title or content')
    show_parser.add_argument('--title', help='Show only articles containing this text in title')

    # export command
    export_parser = subparsers.add_parser('export', help='Export scraped articles from database')
    export_parser.add_argument('spider_name', help='Spider name to export articles from')
    export_parser.add_argument('--format', '-f', choices=['csv', 'json', 'jsonl', 'parquet'], required=True, help='Export format (csv, json, jsonl, or parquet)')
    export_parser.add_argument('--output', '-o', help='Output file path (default: data/<spider_name>_export.<format>)')
    export_parser.add_argument('--limit', '-l', type=int, help='Limit number of articles to export')
    export_parser.add_argument('--url', help='Export only articles containing this URL pattern')
    export_parser.add_argument('--text', '-t', help='Export only articles containing this text in title or content')
    export_parser.add_argument('--title', help='Export only articles containing this text in title')

    # extract-urls command
    extract_urls_parser = subparsers.add_parser('extract-urls', help='Extract all URLs from an HTML file')
    extract_urls_parser.add_argument('--file', type=str, required=True, help='Path to HTML file')
    extract_urls_parser.add_argument('--output', '-o', type=str, help='Output file path (optional, prints to stdout if not specified)')

    # setup command
    subparsers.add_parser('setup', help='Setup virtual environment and database')

    # verify command
    subparsers.add_parser('verify', help='Verify environment setup (no installations)')

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Handle commands
    if args.command == 'setup':
        handle_setup_command()
    elif args.command == 'verify':
        handle_verify_command()
    elif args.command == 'projects':
        handle_projects_command(args)
    elif args.command == 'spiders':
        handle_spiders_command(args)
    elif args.command == 'queue':
        handle_queue_command(args)
    elif args.command == 'db':
        handle_db_command(args)
    elif args.command == 'show':
        handle_show_command(args)
    elif args.command == 'export':
        handle_export_command(args)
    elif args.command == 'extract-urls':
        handle_extract_urls_command(args)
    elif args.command == 'crawl':
        run_spider(args.project, args.spider, args.output, args.limit)
    elif args.command == 'crawl-all':
        run_all_spiders(args.project, args.limit)
    elif args.command == 'list':
        list_spiders(args.project)
    elif args.command == 'test':
        test_spider(args.project, args.spider, args.limit)
    elif args.command == 'status':
        show_project_status(args.project)
    elif args.command == 'logs':
        show_project_logs(args.project, args.spider)

def handle_setup_command():
    """Handle setup command - automated virtual environment and database setup"""
    import subprocess
    import sys
    from pathlib import Path
    
    print("ğŸš€ Setting up ScrapAI environment...")
    
    # Check if virtual environment exists
    venv_path = Path('.venv')
    if not venv_path.exists():
        print("ğŸ“¦ Creating virtual environment...")
        try:
            subprocess.run([sys.executable, '-m', 'venv', '.venv'], check=True)
            print("âœ… Virtual environment created")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to create virtual environment: {e}")
            sys.exit(1)
    else:
        print("âœ… Virtual environment already exists")
    
    # Check if we're in a virtual environment
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âš ï¸  Virtual environment not activated")
        print("ğŸ”„ Run: source .venv/bin/activate")
        print("ğŸ”„ Then run: ./scrapai setup")
        return
    
    # Install requirements if needed
    requirements_path = Path('requirements.txt')
    if requirements_path.exists():
        print("ğŸ“‹ Installing requirements...")
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)
            print("âœ… Requirements installed")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install requirements: {e}")
            sys.exit(1)
    else:
        print("âš ï¸  requirements.txt not found")
    
    # Check database setup
    print("ğŸ—„ï¸  Checking database setup...")
    try:
        # Try to check current migration state
        result = subprocess.run(['./scrapai', 'db', 'current'], capture_output=True, text=True)
        if result.returncode == 0 and 'head' in result.stdout:
            print("âœ… Database already initialized")
        else:
            print("ğŸ”§ Initializing database...")
            subprocess.run(['python', './init_db.py'], check=True)
            print("âœ… Database initialized")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Database setup failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Setup error: {e}")
        sys.exit(1)
    
    print("ğŸ‰ ScrapAI setup complete!")
    print("ğŸ“ You can now:")
    print("   â€¢ List spiders: ./scrapai spiders list")
    print("   â€¢ Import spiders: ./scrapai spiders import <file>")
    print("   â€¢ Run crawls: ./scrapai crawl <spider_name>")

def handle_verify_command():
    """Verify environment setup without installing anything"""
    import subprocess
    import sys
    from pathlib import Path

    print("ğŸ” Verifying ScrapAI environment...\n")

    all_good = True

    # Check if virtual environment exists
    venv_path = Path('.venv')
    if not venv_path.exists():
        print("âŒ Virtual environment not found")
        print("   Run: ./scrapai setup")
        all_good = False
    else:
        print("âœ… Virtual environment exists")

    # Check if we're in a virtual environment
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âŒ Virtual environment not activated")
        print("   Run: source .venv/bin/activate")
        all_good = False
    else:
        print("âœ… Virtual environment activated")

        # Check if key dependencies are installed (only if venv is activated)
        try:
            import scrapy
            import sqlalchemy
            import alembic
            print("âœ… Core dependencies installed")
        except ImportError as e:
            print(f"âŒ Missing dependencies: {e}")
            print("   Run: source .venv/bin/activate && ./scrapai setup")
            all_good = False

    # Check database setup (only if venv is activated and dependencies exist)
    if all_good:
        try:
            result = subprocess.run(
                [sys.executable, '-m', 'alembic', 'current'],
                capture_output=True,
                text=True,
                cwd=os.path.dirname(os.path.abspath(__file__))
            )
            if result.returncode == 0:
                if 'head' in result.stdout or result.stdout.strip():
                    print("âœ… Database initialized")
                else:
                    print("âŒ Database not initialized")
                    print("   Run: source .venv/bin/activate && ./scrapai setup")
                    all_good = False
            else:
                print("âŒ Unable to check database status")
                print("   Run: source .venv/bin/activate && ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"âŒ Error checking database: {e}")
            print("   Run: source .venv/bin/activate && ./scrapai setup")
            all_good = False

    print()
    if all_good:
        print("ğŸ‰ Environment is ready!")
        print("ğŸ“ You can now:")
        print("   â€¢ List spiders: ./scrapai spiders list")
        print("   â€¢ Import spiders: ./scrapai spiders import <file>")
        print("   â€¢ Run crawls: ./scrapai crawl <spider_name>")
    else:
        print("âš ï¸  Environment setup incomplete")
        print("   Run: ./scrapai setup")

def handle_spiders_command(args):
    """Handle spider management commands"""
    from core.db import get_db
    from core.models import Spider, SpiderRule, SpiderSetting
    import json
    
    db = next(get_db())
    
    if args.spiders_command == 'list':
        spiders = db.query(Spider).all()
        if spiders:
            print("ğŸ“‹ Available Spiders (DB):")
            for s in spiders:
                created = s.created_at.strftime('%Y-%m-%d %H:%M') if s.created_at else 'Unknown'
                updated = s.updated_at.strftime('%Y-%m-%d %H:%M') if hasattr(s, 'updated_at') and s.updated_at else created
                print(f"  â€¢ {s.name} (Active: {s.active}) - Created: {created}, Updated: {updated}")
        else:
            print("No spiders found in database.")
            
    elif args.spiders_command == 'import':
        try:
            if args.file == '-':
                # Read from stdin
                import sys
                data = json.load(sys.stdin)
            else:
                # Read from file
                with open(args.file, 'r') as f:
                    data = json.load(f)
            
            # Check if spider exists
            existing = db.query(Spider).filter(Spider.name == data['name']).first()
            if existing:
                print(f"âš ï¸  Spider '{data['name']}' already exists. Updating...")
                # Simple update: delete and recreate (or just update fields)
                # For now, let's just update the main fields and replace rules
                existing.allowed_domains = data['allowed_domains']
                existing.start_urls = data['start_urls']
                
                # Clear old rules and settings
                db.query(SpiderRule).filter(SpiderRule.spider_id == existing.id).delete()
                db.query(SpiderSetting).filter(SpiderSetting.spider_id == existing.id).delete()
                spider = existing
            else:
                spider = Spider(
                    name=data['name'],
                    allowed_domains=data['allowed_domains'],
                    start_urls=data['start_urls']
                )
                db.add(spider)
                db.flush() # Get ID
            
            # Add Rules
            for r_data in data.get('rules', []):
                rule = SpiderRule(
                    spider_id=spider.id,
                    allow_patterns=r_data.get('allow'),
                    deny_patterns=r_data.get('deny'),
                    restrict_xpaths=r_data.get('restrict_xpaths'),
                    restrict_css=r_data.get('restrict_css'),
                    callback=r_data.get('callback'),
                    follow=r_data.get('follow', True),
                    priority=r_data.get('priority', 0)
                )
                db.add(rule)
            
            # Add Settings
            for k, v in data.get('settings', {}).items():
                setting = SpiderSetting(
                    spider_id=spider.id,
                    key=k,
                    value=str(v),
                    type=type(v).__name__
                )
                db.add(setting)
            
            db.commit()
            print(f"âœ… Spider '{spider.name}' imported successfully!")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ Error importing spider: {e}")
            
    elif args.spiders_command == 'delete':
        spider = db.query(Spider).filter(Spider.name == args.name).first()
        if spider:
            if not args.force:
                confirm = input(f"Are you sure you want to delete spider '{args.name}'? (y/N): ")
                if confirm.lower() != 'y':
                    print("âŒ Delete cancelled")
                    return
            
            db.delete(spider)
            db.commit()
            print(f"ğŸ—‘ï¸  Spider '{args.name}' deleted!")
        else:
            print(f"âŒ Spider '{args.name}' not found.")

def handle_queue_command(args):
    """Handle queue management commands"""
    from core.db import get_db
    from core.models import CrawlQueue
    from datetime import datetime
    import socket
    import getpass
    from sqlalchemy import text

    db = next(get_db())

    if args.queue_command == 'add':
        # Check if URL already exists in queue for this project
        existing = db.query(CrawlQueue).filter(
            CrawlQueue.project_name == args.project,
            CrawlQueue.website_url == args.url
        ).first()

        if existing:
            status_emoji = {
                'pending': 'â³',
                'processing': 'ğŸ”„',
                'completed': 'âœ…',
                'failed': 'âŒ'
            }.get(existing.status, 'â“')

            print(f"âš ï¸  URL already exists in queue")
            print(f"   {status_emoji} ID: {existing.id}")
            print(f"   Status: {existing.status}")
            print(f"   URL: {existing.website_url}")
            print(f"   Skipping duplicate...")
            return

        # Add new item to queue
        queue_item = CrawlQueue(
            project_name=args.project,
            website_url=args.url,
            custom_instruction=args.custom_instruction,
            priority=args.priority
        )
        db.add(queue_item)
        db.commit()

        print(f"âœ… Added to queue (ID: {queue_item.id})")
        print(f"   URL: {args.url}")
        print(f"   Project: {args.project}")
        print(f"   Priority: {args.priority}")
        if args.custom_instruction:
            print(f"   Instructions: {args.custom_instruction}")

    elif args.queue_command == 'list':
        # List queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.status:
            # If status is explicitly specified, filter by that status
            query = query.filter(CrawlQueue.status == args.status)
        elif not args.all:
            # By default, exclude failed and completed items (show only pending and processing)
            query = query.filter(CrawlQueue.status.in_(['pending', 'processing']))

        query = query.order_by(CrawlQueue.priority.desc(), CrawlQueue.created_at.asc())

        if args.limit:
            query = query.limit(args.limit)

        items = query.all()

        if not items:
            status_msg = f" with status '{args.status}'" if args.status else ""
            print(f"ğŸ“‹ No items in queue for project '{args.project}'{status_msg}")
            return

        print(f"ğŸ“‹ Queue for project '{args.project}':")
        print()

        for item in items:
            status_emoji = {
                'pending': 'â³',
                'processing': 'ğŸ”„',
                'completed': 'âœ…',
                'failed': 'âŒ'
            }.get(item.status, 'â“')

            print(f"{status_emoji} [{item.id}] {item.website_url}")
            print(f"   Status: {item.status} | Priority: {item.priority}")

            if item.custom_instruction:
                print(f"   Instructions: {item.custom_instruction}")

            if item.processing_by:
                locked_time = item.locked_at.strftime('%Y-%m-%d %H:%M') if item.locked_at else 'Unknown'
                print(f"   Processing by: {item.processing_by} (since {locked_time})")

            if item.error_message:
                print(f"   Error: {item.error_message}")

            if item.completed_at:
                completed_time = item.completed_at.strftime('%Y-%m-%d %H:%M')
                print(f"   Completed: {completed_time}")

            print()

    elif args.queue_command == 'next':
        # Atomic claim next item using PostgreSQL FOR UPDATE SKIP LOCKED
        processing_by = f"{getpass.getuser()}@{socket.gethostname()}"

        # Use raw SQL for atomic operation
        result = db.execute(text("""
            UPDATE crawl_queue
            SET
                status = 'processing',
                processing_by = :processing_by,
                locked_at = NOW(),
                updated_at = NOW()
            WHERE id = (
                SELECT id
                FROM crawl_queue
                WHERE status = 'pending'
                  AND project_name = :project_name
                ORDER BY priority DESC, created_at ASC
                LIMIT 1
                FOR UPDATE SKIP LOCKED
            )
            RETURNING id, website_url, custom_instruction, priority
        """), {
            'processing_by': processing_by,
            'project_name': args.project
        })

        db.commit()

        row = result.fetchone()

        if row:
            print(f"ğŸ”„ Claimed item from queue:")
            print(f"   ID: {row[0]}")
            print(f"   URL: {row[1]}")
            if row[2]:
                print(f"   Instructions: {row[2]}")
            print(f"   Priority: {row[3]}")
            print(f"   Locked by: {processing_by}")
        else:
            print(f"ğŸ“­ No pending items in queue for project '{args.project}'")

    elif args.queue_command == 'complete':
        # Mark item as completed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"âŒ Queue item {args.id} not found")
            return

        from datetime import timezone
        now = datetime.now(timezone.utc)
        item.status = 'completed'
        item.completed_at = now
        item.updated_at = now
        db.commit()

        print(f"âœ… Item {args.id} marked as completed")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'fail':
        # Mark item as failed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"âŒ Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'failed'
        item.error_message = args.error_message
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"âŒ Item {args.id} marked as failed")
        print(f"   URL: {item.website_url}")
        if args.error_message:
            print(f"   Error: {args.error_message}")

    elif args.queue_command == 'retry':
        # Retry a failed item
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"âŒ Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'pending'
        item.retry_count += 1
        item.error_message = None
        item.processing_by = None
        item.locked_at = None
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"ğŸ”„ Item {args.id} reset to pending (retry count: {item.retry_count})")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'remove':
        # Remove item from queue
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"âŒ Queue item {args.id} not found")
            return

        url = item.website_url
        db.delete(item)
        db.commit()

        print(f"ğŸ—‘ï¸  Item {args.id} removed from queue")
        print(f"   URL: {url}")

    elif args.queue_command == 'cleanup':
        # Bulk cleanup queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.all:
            # Remove all completed and failed items
            query = query.filter(CrawlQueue.status.in_(['completed', 'failed']))
        elif args.completed:
            # Remove only completed items
            query = query.filter(CrawlQueue.status == 'completed')
        elif args.failed:
            # Remove only failed items
            query = query.filter(CrawlQueue.status == 'failed')
        else:
            print("âŒ Please specify --completed, --failed, or --all")
            return

        items = query.all()

        if not items:
            status_filter = "all completed and failed" if args.all else ("completed" if args.completed else "failed")
            print(f"ğŸ“‹ No {status_filter} items to cleanup in project '{args.project}'")
            return

        # Show what will be deleted
        print(f"ğŸ—‘ï¸  Found {len(items)} items to remove:")
        for item in items[:5]:  # Show first 5
            status_emoji = 'âœ…' if item.status == 'completed' else 'âŒ'
            print(f"   {status_emoji} [{item.id}] {item.website_url}")
        if len(items) > 5:
            print(f"   ... and {len(items) - 5} more")

        # Confirm unless --force
        if not args.force:
            confirm = input(f"\nRemove {len(items)} items? (y/N): ")
            if confirm.lower() != 'y':
                print("âŒ Cleanup cancelled")
                return

        # Delete items
        for item in items:
            db.delete(item)
        db.commit()

        print(f"âœ… Removed {len(items)} items from queue")

def handle_db_command(args):
    """Handle database management commands"""
    import subprocess
    import sys
    import os
    
    if args.db_command == 'migrate':
        print("ğŸ”„ Running database migrations...")
        try:
            result = subprocess.run([
                sys.executable, '-m', 'alembic', 'upgrade', 'head'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
            if result.returncode == 0:
                print("âœ… Migrations completed successfully!")
            else:
                print("âŒ Migration failed!")
        except Exception as e:
            print(f"âŒ Error running migrations: {e}")
            
    elif args.db_command == 'current':
        try:
            subprocess.run([
                sys.executable, '-m', 'alembic', 'current'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
        except Exception as e:
            print(f"âŒ Error checking current revision: {e}")

def handle_show_command(args):
    """Show scraped articles from database"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from datetime import datetime

    db = next(get_db())

    # Find spider by name
    spider = db.query(Spider).filter(Spider.name == args.spider_name).first()
    if not spider:
        print(f"âŒ Spider '{args.spider_name}' not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()

    if not items:
        print(f"ğŸ“­ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    print(f"ğŸ“° Showing {len(items)} articles from '{args.spider_name}':")
    if filters_applied:
        print(f"   (filtered by: {', '.join(filters_applied)})")
    print()

    for i, item in enumerate(items, 1):
        # Format date nicely
        scraped_date = item.scraped_at.strftime('%Y-%m-%d %H:%M') if item.scraped_at else 'Unknown'
        pub_date = item.published_date.strftime('%Y-%m-%d') if item.published_date else 'Unknown'

        print(f"ğŸ”¸ [{i}] {item.title or 'No Title'}")
        print(f"   ğŸ“… Published: {pub_date} | Scraped: {scraped_date}")
        print(f"   ğŸ”— {item.url}")
        if item.author:
            print(f"   âœï¸  {item.author}")

        # Show content preview (first 150 chars)
        if item.content:
            content_preview = item.content[:150].replace('\n', ' ').strip()
            if len(item.content) > 150:
                content_preview += "..."
            print(f"   ğŸ“ {content_preview}")
        print()

def handle_export_command(args):
    """Export scraped articles from database to various formats"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from pathlib import Path
    import csv
    import json

    db = next(get_db())

    # Find spider by name
    spider = db.query(Spider).filter(Spider.name == args.spider_name).first()
    if not spider:
        print(f"âŒ Spider '{args.spider_name}' not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    if args.limit:
        items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()
    else:
        items = query.order_by(ScrapedItem.scraped_at.desc()).all()

    if not items:
        print(f"ğŸ“­ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    # Determine output file path
    if args.output:
        output_path = Path(args.output)
    else:
        # Create data directory if it doesn't exist
        from datetime import datetime
        data_dir = Path('data')
        data_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%d%m%Y_%H%M%S')
        output_path = data_dir / f"{args.spider_name}_export_{timestamp}.{args.format}"

    # Ensure parent directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert items to list of dicts
    items_data = []
    for item in items:
        item_dict = {
            'id': item.id,
            'url': item.url,
            'title': item.title,
            'content': item.content,
            'author': item.author,
            'published_date': item.published_date.isoformat() if item.published_date else None,
            'scraped_at': item.scraped_at.isoformat() if item.scraped_at else None,
            'metadata': item.metadata_json
        }
        items_data.append(item_dict)

    try:
        # Export based on format
        if args.format == 'csv':
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                if items_data:
                    writer = csv.DictWriter(f, fieldnames=items_data[0].keys())
                    writer.writeheader()
                    writer.writerows(items_data)
            print(f"âœ… Exported {len(items_data)} articles to CSV: {output_path}")

        elif args.format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(items_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… Exported {len(items_data)} articles to JSON: {output_path}")

        elif args.format == 'jsonl':
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in items_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"âœ… Exported {len(items_data)} articles to JSONL: {output_path}")

        elif args.format == 'parquet':
            try:
                import pandas as pd
                df = pd.DataFrame(items_data)
                df.to_parquet(output_path, index=False)
                print(f"âœ… Exported {len(items_data)} articles to Parquet: {output_path}")
            except ImportError:
                print("âŒ Parquet export requires pandas and pyarrow libraries.")
                print("   Run: pip install pandas pyarrow")
                return

        if filters_applied:
            print(f"   (filtered by: {', '.join(filters_applied)})")

    except Exception as e:
        print(f"âŒ Error exporting data: {e}")

def handle_extract_urls_command(args):
    """Extract all URLs from an HTML file"""
    from utils.url_extractor import extract_urls_from_html

    try:
        urls = extract_urls_from_html(args.file, args.output)

        # If no output file specified, print to stdout
        if not args.output:
            print('\n'.join(urls))
        else:
            print(f"âœ… Extracted {len(urls)} URLs to {args.output}")

    except FileNotFoundError as e:
        print(f"âŒ {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Error: {e}")
        sys.exit(1)

def handle_projects_command(args):
    """Handle project management commands"""
    pm = ProjectManager()
    
    if args.projects_command == 'create':
        spiders = [s.strip() for s in args.spiders.split(',')]
        pm.create_project(args.name, spiders)
    elif args.projects_command == 'list':
        projects = pm.list_projects()
        if projects:
            print("ğŸ“‹ Available projects:")
            for project in projects:
                print(f"  â€¢ {project}")
        else:
            print("No projects found. Create one with: ./scrapai projects create --name <name> --spiders <spider1,spider2>")
    elif args.projects_command == 'status':
        status = pm.get_project_status(args.project)
        if 'error' in status:
            print(f"âŒ {status['error']}")
        else:
            print(f"ğŸ“Š Project: {status['name']}")
            print(f"ğŸ•·ï¸  Spiders: {', '.join(status['spiders'])}")
            print(f"ğŸ“„ Output files: {status['output_files']}")
            print(f"ğŸ“ Log files: {status['log_files']}")
            print(f"ğŸ“ Path: {status['path']}")
    elif args.projects_command == 'delete':
        confirm = input(f"Are you sure you want to delete project '{args.project}'? (y/N): ")
        if confirm.lower() == 'y':
            pm.delete_project(args.project)
        else:
            print("âŒ Delete cancelled")

def run_spider(project_name, spider_name, output_file=None, limit=None):
    """Run a Scrapy spider (DB or File-based)"""
    # Try to find spider in DB first
    from core.db import get_db
    from core.models import Spider
    
    db = next(get_db())
    db_spider = db.query(Spider).filter(Spider.name == spider_name).first()
    
    if db_spider:
        print(f"ğŸš€ Running DB spider: {spider_name}")
        import sys
        from datetime import datetime
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', 'database_spider', '-a', f'spider_name={spider_name}']

        # Smart storage logic based on --limit flag
        if limit:
            # Testing mode: Save to database for verification with `show` command
            print(f"ğŸ§ª Test mode: Saving to database (limit: {limit} items)")
            print(f"   Use './scrapai show {spider_name}' to verify results")
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
            # Don't include HTML in test mode (saves DB storage)
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=False'])
        else:
            # Production mode: Export to files, disable database to save costs
            print(f"ğŸ“ Production mode: Exporting to files (database disabled)")

            # Disable DatabasePipeline to avoid DB costs
            cmd.extend(['-s', 'ITEM_PIPELINES={"pipelines.ScrapaiPipeline": 300}'])

            # Include HTML in JSONL output for full scraping
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=True'])

            # Add default output file if not specified
            if not output_file:
                import os
                now = datetime.now()
                date_folder = now.strftime('%Y-%m-%d')
                output_dir = f'data/{spider_name}/{date_folder}'
                os.makedirs(output_dir, exist_ok=True)
                timestamp = now.strftime('%H%M%S')
                output_file = f'{output_dir}/crawl_{timestamp}.jsonl'

            cmd.extend(['-o', output_file])
            print(f"   Output: {output_file} (includes HTML)")

        # Add explicit output file if provided
        if output_file and limit:
            cmd.extend(['-o', output_file])
            print(f"   Also saving to: {output_file}")

        subprocess.run(cmd)
        return

    # Fallback to legacy file-based project system
    if not project_name:
        print(f"âŒ Spider '{spider_name}' not found in DB. To run legacy file-based spider, please provide --project.")
        return

    config_loader = ConfigLoader()
    
    try:
        # Validate project and spider
        if not config_loader.validate_spider_for_project(project_name, spider_name):
            print(f"âŒ Spider '{spider_name}' not found in DB and not configured for project '{project_name}'")
            return
        
        # Get project settings
        settings = config_loader.get_spider_settings(project_name, spider_name)
        
        # Build scrapy command
        import sys
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', spider_name]
        
        # Add settings (only simple ones that can be passed via command line)
        for key, value in settings.items():
            if key not in ['FEEDS', 'LOG_FILE', 'ITEM_PIPELINES']:  # These are handled differently
                # Only add simple settings that can be passed via command line
                if isinstance(value, (str, int, float, bool)):
                    cmd.extend(['-s', f'{key}={value}'])
        
        # Handle output
        if output_file:
            cmd.extend(['-o', output_file])
        else:
            # Create output directory and add default output
            import os
            output_dir = f'projects/{project_name}/outputs/{spider_name}'
            os.makedirs(output_dir, exist_ok=True)
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'{output_dir}/{timestamp}.json'
            cmd.extend(['-o', output_file])
        
        # Handle log file
        if 'LOG_FILE' in settings:
            cmd.extend(['-s', f'LOG_FILE={settings["LOG_FILE"]}'])
        
        if limit:
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
        
        print(f"ğŸš€ Running spider: {spider_name} (project: {project_name})")
        subprocess.run(cmd)
        
    except Exception as e:
        print(f"âŒ Error running spider: {e}")

def run_all_spiders(project_name, limit=None):
    """Run all spiders configured for a project"""
    config_loader = ConfigLoader()
    
    try:
        spiders = config_loader.get_project_spiders(project_name)
        if not spiders:
            print(f"âŒ No spiders configured for project '{project_name}'")
            return
        
        print(f"ğŸš€ Running all spiders for project: {project_name}")
        print(f"ğŸ•·ï¸  Spiders: {', '.join(spiders)}")
        
        for spider in spiders:
            print(f"\n{'='*50}")
            print(f"Running: {spider}")
            print(f"{'='*50}")
            run_spider(project_name, spider, None, limit)
            
    except Exception as e:
        print(f"âŒ Error running spiders: {e}")

def test_spider(project_name, spider_name, limit=5):
    """Test a spider with limited items"""
    print(f"ğŸ§ª Testing spider: {spider_name} (project: {project_name}, limit: {limit})")
    run_spider(project_name, spider_name, None, limit)

def list_spiders(project_name=None):
    """List available spiders"""
    if project_name:
        # List spiders for specific project
        config_loader = ConfigLoader()
        try:
            spiders = config_loader.get_project_spiders(project_name)
            print(f"ğŸ“‹ Spiders configured for project '{project_name}':")
            for spider in spiders:
                print(f"  â€¢ {spider}")
        except Exception as e:
            print(f"âŒ Error: {e}")
    else:
        # List all available spiders
        print("ğŸ“‹ Available spiders:")
        spiders_dir = Path("spiders")
        if spiders_dir.exists():
            for spider_file in spiders_dir.glob("*.py"):
                if spider_file.name != "__init__.py" and spider_file.name != "base_spider.py":
                    spider_name = spider_file.stem
                    print(f"  â€¢ {spider_name}")
        else:
            print("  No spiders found. Ask Claude Code to create some!")

def show_project_status(project_name):
    """Show project status"""
    pm = ProjectManager()
    status = pm.get_project_status(project_name)
    
    if 'error' in status:
        print(f"âŒ {status['error']}")
    else:
        print(f"ğŸ“Š Project: {status['name']}")
        print(f"ğŸ•·ï¸  Spiders: {', '.join(status['spiders'])}")
        print(f"ğŸ“„ Output files: {status['output_files']}")
        print(f"ğŸ“ Log files: {status['log_files']}")
        print(f"ğŸ“ Path: {status['path']}")

def show_project_logs(project_name, spider_name=None):
    """Show project logs"""
    pm = ProjectManager()
    
    if not pm.project_exists(project_name):
        print(f"âŒ Project '{project_name}' not found")
        return
    
    logs_dir = pm.get_project_path(project_name) / "logs"
    
    if not logs_dir.exists():
        print(f"ğŸ“ No logs found for project '{project_name}'")
        return
    
    if spider_name:
        # Show specific spider logs
        log_file = logs_dir / f"{spider_name}.log"
        if log_file.exists():
            print(f"ğŸ“ Logs for {spider_name} (project: {project_name}):")
            print("-" * 50)
            with open(log_file, 'r') as f:
                print(f.read())
        else:
            print(f"âŒ No logs found for spider '{spider_name}' in project '{project_name}'")
    else:
        # Show all log files
        log_files = list(logs_dir.glob("*.log"))
        if log_files:
            print(f"ğŸ“ Log files for project '{project_name}':")
            for log_file in log_files:
                print(f"  â€¢ {log_file.name}")
        else:
            print(f"ğŸ“ No log files found for project '{project_name}'")

if __name__ == '__main__':
    main()