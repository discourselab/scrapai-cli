#!/usr/bin/env python3
"""
scrapai - Simple Scrapy CLI for running Claude Code generated spiders with project support
"""

import argparse
import subprocess
import sys
import os
from pathlib import Path

def auto_activate_venv():
    """
    Auto-activate virtual environment by re-executing script with venv Python.
    This makes the CLI work transparently without requiring manual venv activation.
    """
    # Skip if already in venv or in special environments
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        return  # Already in venv

    if os.getenv('SKIP_VENV_CHECK') or os.getenv('AIRFLOW_HOME'):
        return  # Skip in Docker/Airflow

    # Look for venv in common locations
    script_dir = os.path.dirname(os.path.abspath(__file__))
    venv_paths = [
        os.path.join(script_dir, '.venv'),
        os.path.join(script_dir, 'venv'),
    ]

    for venv_path in venv_paths:
        venv_python = os.path.join(venv_path, 'bin', 'python')
        if os.path.exists(venv_python):
            # Re-execute this script with venv Python
            args = [venv_python] + sys.argv
            os.execv(venv_python, args)
            # execv replaces current process, so this line never runs
            return

    # No venv found - continue with system Python (setup will create one)

# Auto-activate venv before anything else (except for setup/verify commands)
if len(sys.argv) > 1 and sys.argv[1] not in ['setup', 'verify', '--help', '-h']:
    auto_activate_venv()

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Only import dependencies if not running setup command or if dependencies are available
try:
    from core.config_loader import ConfigLoader
except ImportError:
    if len(sys.argv) <= 1 or (sys.argv[1] not in ['setup', 'verify', '--help', '-h']):
        # If it's not setup/verify/help command and imports fail, show error
        print("‚ùå Dependencies not installed. Run './scrapai setup' first.")
        sys.exit(1)
    # For setup/verify/help command, continue without imports
    ConfigLoader = None

def main():
    
    parser = argparse.ArgumentParser(description='Run Scrapy spiders with project support')
    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # crawl command
    crawl_parser = subparsers.add_parser('crawl', help='Run a spider')
    crawl_parser.add_argument('--project', help='Project name (optional for DB spiders)')
    crawl_parser.add_argument('spider', help='Spider name to run')
    crawl_parser.add_argument('--output', '-o', help='Output file path (optional, uses project config)')
    crawl_parser.add_argument('--limit', '-l', type=int, help='Limit number of items')
    crawl_parser.add_argument('--timeout', '-t', type=int, help='Max runtime in seconds (graceful stop, e.g. 28800 for 8h)')
    
    # crawl-all command
    crawl_all_parser = subparsers.add_parser('crawl-all', help='Run all spiders in a project')
    crawl_all_parser.add_argument('--project', required=True, help='Project name')
    crawl_all_parser.add_argument('--limit', '-l', type=int, help='Limit number of items per spider')

    # spiders command
    spiders_parser = subparsers.add_parser('spiders', help='Spider management')
    spiders_subparsers = spiders_parser.add_subparsers(dest='spiders_command', help='Spider commands')
    
    # spiders list
    list_spiders_parser = spiders_subparsers.add_parser('list', help='List all spiders in DB')
    list_spiders_parser.add_argument('--project', help='Filter by project name (default: show all)')
    
    # spiders import
    import_parser = spiders_subparsers.add_parser('import', help='Import spider from JSON')
    import_parser.add_argument('file', help='JSON file containing spider definition (use "-" to read from stdin)')
    import_parser.add_argument('--project', default='default', help='Project name (default: default)')
    
    # spiders delete
    delete_parser = spiders_subparsers.add_parser('delete', help='Delete a spider')
    delete_parser.add_argument('name', help='Spider name')
    delete_parser.add_argument('--project', help='Project name (optional - if not specified, deletes first match)')
    delete_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # projects command
    projects_parser = subparsers.add_parser('projects', help='Project management')
    projects_subparsers = projects_parser.add_subparsers(dest='projects_command', help='Project commands')

    # projects list
    list_projects_parser = projects_subparsers.add_parser('list', help='List all projects')

    # queue command for queue management
    queue_parser = subparsers.add_parser('queue', help='Queue management')
    queue_subparsers = queue_parser.add_subparsers(dest='queue_command', help='Queue commands')

    # queue add
    add_parser = queue_subparsers.add_parser('add', help='Add website to queue')
    add_parser.add_argument('url', help='Website URL to add to queue')
    add_parser.add_argument('-m', '--message', dest='custom_instruction', help='Custom instruction for processing')
    add_parser.add_argument('--priority', type=int, default=5, help='Priority (higher = sooner, default: 5)')
    add_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue list
    list_parser = queue_subparsers.add_parser('list', help='List queue items (excludes failed/completed by default)')
    list_parser.add_argument('--project', default='default', help='Project name (default: default)')
    list_parser.add_argument('--status', help='Filter by status (pending, processing, completed, failed)')
    list_parser.add_argument('--limit', type=int, default=5, help='Limit number of items to show (default: 5)')
    list_parser.add_argument('--all', action='store_true', help='Show all items including failed and completed')
    list_parser.add_argument('--count', action='store_true', help='Show only the count of items')

    # queue next
    next_parser = queue_subparsers.add_parser('next', help='Get next item from queue (atomic claim)')
    next_parser.add_argument('--project', default='default', help='Project name (default: default)')

    # queue complete
    complete_parser = queue_subparsers.add_parser('complete', help='Mark item as completed')
    complete_parser.add_argument('id', type=int, help='Queue item ID')

    # queue fail
    fail_parser = queue_subparsers.add_parser('fail', help='Mark item as failed')
    fail_parser.add_argument('id', type=int, help='Queue item ID')
    fail_parser.add_argument('-m', '--message', dest='error_message', help='Error message')

    # queue retry
    retry_parser = queue_subparsers.add_parser('retry', help='Retry a failed item')
    retry_parser.add_argument('id', type=int, help='Queue item ID')

    # queue remove
    remove_parser = queue_subparsers.add_parser('remove', help='Remove item from queue')
    remove_parser.add_argument('id', type=int, help='Queue item ID')

    # queue cleanup
    cleanup_parser = queue_subparsers.add_parser('cleanup', help='Bulk cleanup queue items')
    cleanup_parser.add_argument('--completed', action='store_true', help='Remove all completed items')
    cleanup_parser.add_argument('--failed', action='store_true', help='Remove all failed items')
    cleanup_parser.add_argument('--all', action='store_true', help='Remove all completed and failed items')
    cleanup_parser.add_argument('--project', default='default', help='Project name (default: default)')
    cleanup_parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation prompt')

    # queue bulk
    bulk_parser = queue_subparsers.add_parser('bulk', help='Bulk add URLs from JSON or CSV file')
    bulk_parser.add_argument('file', help='JSON or CSV file containing URLs (JSON: array of objects with "url" field, CSV: columns "url", optional "name"/"custom_instruction", "priority")')
    bulk_parser.add_argument('--project', default='default', help='Project name (default: default)')
    bulk_parser.add_argument('--priority', type=int, default=5, help='Default priority for items without priority column (default: 5)')

    # db command for database operations
    db_parser = subparsers.add_parser('db', help='Database management')
    db_subparsers = db_parser.add_subparsers(dest='db_command', help='Database commands')

    # db migrate
    db_subparsers.add_parser('migrate', help='Run database migrations')

    # db current
    db_subparsers.add_parser('current', help='Show current migration revision')

    # show command
    show_parser = subparsers.add_parser('show', help='Show scraped articles from database')
    show_parser.add_argument('spider_name', help='Spider name to show articles from')
    show_parser.add_argument('--project', help='Project name (optional - if not specified, shows first match)')
    show_parser.add_argument('--limit', '-l', type=int, default=5, help='Number of articles to show (default: 5)')
    show_parser.add_argument('--url', help='Show only articles containing this URL pattern')
    show_parser.add_argument('--text', '-t', help='Show only articles containing this text in title or content')
    show_parser.add_argument('--title', help='Show only articles containing this text in title')

    # export command
    export_parser = subparsers.add_parser('export', help='Export scraped articles from database')
    export_parser.add_argument('spider_name', help='Spider name to export articles from')
    export_parser.add_argument('--project', help='Project name (optional - if not specified, exports first match)')
    export_parser.add_argument('--format', '-f', choices=['csv', 'json', 'jsonl', 'parquet'], required=True, help='Export format (csv, json, jsonl, or parquet)')
    export_parser.add_argument('--output', '-o', help='Output file path (default: data/<spider_name>_export.<format>)')
    export_parser.add_argument('--limit', '-l', type=int, help='Limit number of articles to export')
    export_parser.add_argument('--url', help='Export only articles containing this URL pattern')
    export_parser.add_argument('--text', '-t', help='Export only articles containing this text in title or content')
    export_parser.add_argument('--title', help='Export only articles containing this text in title')

    # extract-urls command
    extract_urls_parser = subparsers.add_parser('extract-urls', help='Extract all URLs from an HTML file')
    extract_urls_parser.add_argument('--file', type=str, required=True, help='Path to HTML file')
    extract_urls_parser.add_argument('--output', '-o', type=str, help='Output file path (optional, prints to stdout if not specified)')

    # setup command
    subparsers.add_parser('setup', help='Setup virtual environment and database')

    # verify command
    subparsers.add_parser('verify', help='Verify environment setup (no installations)')

    # inspect command
    inspect_parser = subparsers.add_parser('inspect', help='Inspect a website to help create scrapers')
    inspect_parser.add_argument('url', help='URL to inspect')
    inspect_parser.add_argument('--project', type=str, default='default', help='Project name for organizing analysis files')
    inspect_parser.add_argument('--output-dir', type=str, default=None, help='Directory to save analysis')
    inspect_parser.add_argument('--proxy-type', choices=['none', 'static', 'residential', 'auto'],
                              default='auto', help='Proxy type to use')
    inspect_parser.add_argument('--no-save-html', action='store_true', help='Do not save the full HTML')
    inspect_parser.add_argument('--cloudflare', action='store_true',
                              help='Use Cloudflare bypass mode (nodriver with CF verification)')
    inspect_parser.add_argument('--log-level', choices=['debug', 'info', 'warning', 'error', 'critical'],
                              default='info', help='Set the logging level')
    inspect_parser.add_argument('--log-file', type=str, help='Path to log file (optional)')

    # analyze command
    analyze_parser = subparsers.add_parser('analyze', help='Analyze HTML for CSS selector discovery')
    analyze_parser.add_argument('html_file', help='Path to HTML file to analyze')
    analyze_parser.add_argument('--test', help='Test a specific CSS selector')
    analyze_parser.add_argument('--find', help='Find elements by keyword (searches classes/ids)')

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Handle commands
    if args.command == 'setup':
        handle_setup_command()
    elif args.command == 'verify':
        handle_verify_command()
    elif args.command == 'inspect':
        handle_inspect_command(args)
    elif args.command == 'analyze':
        handle_analyze_command(args)
    elif args.command == 'spiders':
        handle_spiders_command(args)
    elif args.command == 'projects':
        handle_projects_command(args)
    elif args.command == 'queue':
        handle_queue_command(args)
    elif args.command == 'db':
        handle_db_command(args)
    elif args.command == 'show':
        handle_show_command(args)
    elif args.command == 'export':
        handle_export_command(args)
    elif args.command == 'extract-urls':
        handle_extract_urls_command(args)
    elif args.command == 'crawl':
        run_spider(args.project, args.spider, args.output, args.limit, args.timeout)
    elif args.command == 'crawl-all':
        run_all_spiders(args.project, args.limit)

def handle_setup_command():
    """Handle setup command - automated virtual environment and database setup"""
    import subprocess
    import sys
    from pathlib import Path

    print("üöÄ Setting up ScrapAI environment...")

    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Check if virtual environment exists
    venv_path = Path('.venv')
    venv_python = venv_path / 'bin' / 'python'

    if not venv_path.exists():
        print("üì¶ Creating virtual environment...")
        try:
            subprocess.run([sys.executable, '-m', 'venv', '.venv'], check=True, cwd=script_dir)
            print("‚úÖ Virtual environment created")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to create virtual environment: {e}")
            sys.exit(1)
    else:
        print("‚úÖ Virtual environment already exists")

    # Install requirements using venv Python directly (no activation needed!)
    requirements_path = Path('requirements.txt')
    if requirements_path.exists():
        print("üìã Installing requirements...")
        try:
            # Use venv's Python directly - no activation needed
            subprocess.run([str(venv_python), '-m', 'pip', 'install', '--upgrade', 'pip'],
                         check=True, cwd=script_dir, capture_output=True)
            subprocess.run([str(venv_python), '-m', 'pip', 'install', '-r', 'requirements.txt'],
                         check=True, cwd=script_dir, capture_output=True)
            print("‚úÖ Requirements installed")

            # Install Playwright browsers
            print("üåê Installing Playwright browsers...")
            subprocess.run([str(venv_python), '-m', 'playwright', 'install'],
                         check=True, cwd=script_dir, capture_output=True)
            print("‚úÖ Playwright browsers installed")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install requirements: {e}")
            sys.exit(1)
    else:
        print("‚ö†Ô∏è  requirements.txt not found")

    # Create .env file if it doesn't exist
    env_file = Path('.env')
    env_example = Path('.env.example')
    if not env_file.exists() and env_example.exists():
        print("üìù Creating .env from .env.example...")
        try:
            import shutil
            shutil.copy(env_example, env_file)
            print("‚úÖ .env file created (using SQLite by default)")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not create .env: {e}")
            print("   You may need to create it manually")

    # Initialize database using venv Python
    try:
        init_db_path = os.path.join(script_dir, 'init_db.py')
        subprocess.run([str(venv_python), init_db_path], check=True, cwd=script_dir)
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Database setup failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Setup error: {e}")
        sys.exit(1)

    # Configure Claude Code settings (if running in Claude Code)
    claude_home = Path.home() / '.claude'
    if claude_home.exists():
        print("üîß Configuring Claude Code permissions...")
        try:
            import json
            settings_dir = Path('.claude')
            settings_dir.mkdir(exist_ok=True)
            settings_file = settings_dir / 'settings.local.json'

            # Default permissions to add
            new_allow = [
                "Read",
                "Write",
                "Edit",
                "Update",
                "Glob",
                "Grep",
                "Bash(./scrapai:*)",
                "Bash(source:*)",
                "Bash(sqlite3:*)",
                "Bash(psql:*)"
            ]
            new_deny = [
                "Edit(scrapai)",
                "Update(scrapai)",
                "Edit(.claude/*)",
                "Update(.claude/*)",
                "Write(**/*.py)",
                "Edit(**/*.py)",
                "Update(**/*.py)",
                "MultiEdit(**/*.py)",
                "Write(.env)",
                "Write(secrets/**)",
                "Write(config/**/*.key)",
                "Write(**/*password*)",
                "Write(**/*secret*)",
                "WebFetch",
                "WebSearch",
                "Bash(rm:*)"
            ]

            # Read existing settings if file exists
            if settings_file.exists():
                with open(settings_file, 'r') as f:
                    settings = json.load(f)
            else:
                settings = {"permissions": {}}

            # Ensure permissions key exists
            if "permissions" not in settings:
                settings["permissions"] = {}

            # Merge allow list (avoid duplicates)
            existing_allow = settings["permissions"].get("allow", [])
            for item in new_allow:
                if item not in existing_allow:
                    existing_allow.append(item)
            settings["permissions"]["allow"] = existing_allow

            # Merge deny list (avoid duplicates)
            existing_deny = settings["permissions"].get("deny", [])
            for item in new_deny:
                if item not in existing_deny:
                    existing_deny.append(item)
            settings["permissions"]["deny"] = existing_deny

            # Write merged settings
            with open(settings_file, 'w') as f:
                json.dump(settings, f, indent=2)

            print("‚úÖ Claude Code permissions configured in .claude/settings.local.json")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not configure Claude Code settings: {e}")
            # Don't fail setup if this fails

    print("üéâ ScrapAI setup complete!")
    print("üìù You can now:")
    print("   ‚Ä¢ List spiders: ./scrapai spiders list --project <name>")
    print("   ‚Ä¢ Import spiders: ./scrapai spiders import <file> --project <name>")
    print("   ‚Ä¢ Run crawls: ./scrapai crawl <spider_name> --project <name>")

def handle_verify_command():
    """Verify environment setup without installing anything"""
    import subprocess
    import sys
    from pathlib import Path

    print("üîç Verifying ScrapAI environment...\n")

    all_good = True
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Check if virtual environment exists
    venv_path = Path('.venv')
    venv_python = venv_path / 'bin' / 'python'

    if not venv_path.exists():
        print("‚ùå Virtual environment not found")
        print("   Run: ./scrapai setup")
        all_good = False
    else:
        print("‚úÖ Virtual environment exists")

        # Check if key dependencies are installed (using venv Python directly)
        try:
            result = subprocess.run(
                [str(venv_python), '-c', 'import scrapy, sqlalchemy, alembic; print("ok")'],
                capture_output=True,
                text=True,
                cwd=script_dir
            )
            if result.returncode == 0 and 'ok' in result.stdout:
                print("‚úÖ Core dependencies installed")
            else:
                print("‚ùå Missing dependencies")
                print("   Run: ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"‚ùå Error checking dependencies: {e}")
            print("   Run: ./scrapai setup")
            all_good = False

    # Check database setup
    if all_good:
        try:
            result = subprocess.run(
                [str(venv_python), '-m', 'alembic', 'current'],
                capture_output=True,
                text=True,
                cwd=script_dir
            )
            if result.returncode == 0:
                if 'head' in result.stdout or result.stdout.strip():
                    print("‚úÖ Database initialized")
                else:
                    print("‚ùå Database not initialized")
                    print("   Run: ./scrapai setup")
                    all_good = False
            else:
                print("‚ùå Unable to check database status")
                print("   Run: ./scrapai setup")
                all_good = False
        except Exception as e:
            print(f"‚ùå Error checking database: {e}")
            print("   Run: ./scrapai setup")
            all_good = False

    print()
    if all_good:
        print("üéâ Environment is ready!")
        print("üìù You can now:")
        print("   ‚Ä¢ List spiders: ./scrapai spiders list --project <name>")
        print("   ‚Ä¢ Import spiders: ./scrapai spiders import <file> --project <name>")
        print("   ‚Ä¢ Run crawls: ./scrapai crawl <spider_name> --project <name>")
    else:
        print("‚ö†Ô∏è  Environment setup incomplete")
        print("   Run: ./scrapai setup")

def handle_inspect_command(args):
    """Handle inspect command - inspect a website"""
    from utils.inspector import inspect_page
    from utils.logger import setup_logging
    import sys

    # Configure logging
    logger = setup_logging(
        'inspector',
        level=args.log_level,
        log_file=args.log_file if hasattr(args, 'log_file') and args.log_file else None
    )

    logger.info(f"Starting inspection of {args.url}")

    # Check display availability if using Cloudflare bypass
    if args.cloudflare:
        from utils.display_helper import needs_xvfb, has_xvfb

        if needs_xvfb():
            if not has_xvfb():
                print("‚ùå ERROR: Cloudflare bypass requires a display")
                print("   No display available and xvfb not installed")
                print("\nOptions:")
                print("   1. Install xvfb: sudo apt-get install xvfb")
                print("   2. Then run: xvfb-run -a ./scrapai inspect <url> --cloudflare")
                print("   3. Or run on a machine with a display")
                sys.exit(1)
            print("üñ•Ô∏è  Headless environment detected - make sure to use: xvfb-run -a ./scrapai inspect ...")
            print("   (If you're seeing this, you may have forgotten to wrap with xvfb-run)")
        else:
            print("üñ•Ô∏è  Display available - using native browser for Cloudflare bypass")

    inspect_page(args.url, args.output_dir, args.proxy_type, not args.no_save_html,
                 use_cloudflare=args.cloudflare, project=args.project)
    logger.info("Inspection complete")

def handle_analyze_command(args):
    """Handle analyze command - analyze HTML for CSS selectors"""
    from bs4 import BeautifulSoup

    def analyze_html(html_path):
        """Analyze HTML and suggest selectors."""
        with open(html_path, 'r', encoding='utf-8') as f:
            html = f.read()

        soup = BeautifulSoup(html, 'lxml')

        print(f"üìÑ Analyzing: {html_path}")
        print(f"üìä HTML size: {len(html)} bytes")
        print("\nüí° TIP: Use --find 'keyword' to search for specific elements (price, rating, etc.)\n")

        # Analyze titles
        print("=" * 60)
        print("üè∑Ô∏è  HEADERS (h1, h2) - Often used for: title, product_name, thread_title")
        print("=" * 60)
        for tag in ['h1', 'h2']:
            elements = soup.find_all(tag)
            if elements:
                print(f"\n{tag.upper()} - Found {len(elements)}:")
                for i, el in enumerate(elements[:5], 1):
                    classes = el.get('class', [])
                    class_str = '.' + '.'.join(classes) if classes else ''
                    text = el.get_text(strip=True)[:80]
                    print(f"  [{i}] {tag}{class_str}")
                    print(f"      Text: {text}")

        # Analyze content containers
        print("\n" + "=" * 60)
        print("üìù CONTENT CONTAINERS (article, div with content-related classes)")
        print("=" * 60)
        content_keywords = ['article', 'content', 'body', 'text', 'post', 'entry']
        found_containers = []

        for el in soup.find_all(['article', 'div', 'section', 'main']):
            classes = el.get('class', [])
            class_str = ' '.join(classes) if classes else ''

            # Check if likely content container
            if el.name == 'article' or any(kw in class_str.lower() for kw in content_keywords):
                text_len = len(el.get_text(strip=True))
                if text_len > 200:  # Substantial content
                    found_containers.append((el, text_len))

        # Sort by text length (largest first)
        found_containers.sort(key=lambda x: x[1], reverse=True)

        for i, (el, text_len) in enumerate(found_containers[:5], 1):
            classes = el.get('class', [])
            class_str = '.' + '.'.join(classes) if classes else ''
            print(f"\n  [{i}] {el.name}{class_str}")
            print(f"      Size: {text_len} chars")
            print(f"      Preview: {el.get_text(strip=True)[:80]}...")

        # Analyze dates
        print("\n" + "=" * 60)
        print("üìÖ DATES (time, elements with date/time classes)")
        print("=" * 60)
        date_keywords = ['date', 'time', 'published', 'posted', 'updated']
        found = 0

        for el in soup.find_all(['time', 'span', 'div', 'p']):
            classes = el.get('class', [])
            class_str = ' '.join(classes) if classes else ''

            if el.name == 'time' or any(kw in class_str.lower() for kw in date_keywords):
                text = el.get_text(strip=True)
                if text and len(text) < 50:  # Reasonable date length
                    classes_list = el.get('class', [])
                    selector = '.' + '.'.join(classes_list) if classes_list else ''
                    print(f"  {el.name}{selector}: {text}")
                    found += 1
                    if found >= 5:
                        break

        # Analyze authors
        print("\n" + "=" * 60)
        print("‚úçÔ∏è  AUTHORS (elements with author/byline classes)")
        print("=" * 60)
        author_keywords = ['author', 'byline', 'writer', 'by']
        found = 0

        for el in soup.find_all(['span', 'div', 'a', 'p']):
            classes = el.get('class', [])
            class_str = ' '.join(classes) if classes else ''

            if any(kw in class_str.lower() for kw in author_keywords):
                text = el.get_text(strip=True)
                if text and len(text) < 100:  # Reasonable author length
                    classes_list = el.get('class', [])
                    selector = '.' + '.'.join(classes_list) if classes_list else ''
                    print(f"  {el.name}{selector}: {text}")
                    found += 1
                    if found >= 5:
                        break

        print("\n" + "=" * 60)

    def test_selector(html_path, selector):
        """Test a CSS selector on HTML."""
        with open(html_path, 'r', encoding='utf-8') as f:
            html = f.read()

        soup = BeautifulSoup(html, 'lxml')

        print(f"\nüîç Testing selector: {selector}")
        print("=" * 60)

        elements = soup.select(selector)

        if not elements:
            print("‚ùå No elements found!")
            return

        print(f"‚úì Found {len(elements)} element(s)\n")

        for i, el in enumerate(elements[:3], 1):
            text = el.get_text(strip=True)
            print(f"[{i}] {el.name}")
            print(f"    Classes: {el.get('class', [])}")
            print(f"    Text ({len(text)} chars): {text[:150]}...")
            print()

    def find_by_keyword(html_path, keyword):
        """Find elements by keyword in classes or text."""
        with open(html_path, 'r', encoding='utf-8') as f:
            html = f.read()

        soup = BeautifulSoup(html, 'lxml')

        print(f"\nüîé Finding elements with keyword: '{keyword}'")
        print("=" * 60)

        found = 0

        # Search in all elements
        for el in soup.find_all():
            # Check classes
            classes = el.get('class', [])
            class_str = ' '.join(classes) if classes else ''

            # Check id
            el_id = el.get('id', '')

            # Match keyword in class or id
            if keyword.lower() in class_str.lower() or keyword.lower() in el_id.lower():
                text = el.get_text(strip=True)
                if text and len(text) < 200:  # Reasonable length
                    classes_list = el.get('class', [])
                    selector = '.' + '.'.join(classes_list) if classes_list else ''
                    if el_id:
                        selector = f"#{el_id}"
                    print(f"\n  {el.name}{selector}")
                    print(f"    Text: {text[:100]}")
                    found += 1
                    if found >= 10:
                        break

        if found == 0:
            print(f"\n‚ùå No elements found with keyword '{keyword}'")
            print("\nüí° Try searching for:")
            print("   - Class keywords: 'price', 'rating', 'author', 'date', 'title'")
            print("   - Content keywords: look at text content in analyze mode")

    # Execute based on flags
    if args.test:
        test_selector(args.html_file, args.test)
    elif args.find:
        find_by_keyword(args.html_file, args.find)
    else:
        analyze_html(args.html_file)

def handle_spiders_command(args):
    """Handle spider management commands"""
    from core.db import get_db
    from core.models import Spider, SpiderRule, SpiderSetting
    import json
    
    db = next(get_db())
    
    if args.spiders_command == 'list':
        query = db.query(Spider)
        if hasattr(args, 'project') and args.project:
            query = query.filter(Spider.project == args.project)
            print(f"üìã Available Spiders (DB) - Project: {args.project}:")
        else:
            print("üìã Available Spiders (DB) - All Projects:")

        spiders = query.all()
        if spiders:
            for s in spiders:
                created = s.created_at.strftime('%Y-%m-%d %H:%M') if s.created_at else 'Unknown'
                updated = s.updated_at.strftime('%Y-%m-%d %H:%M') if hasattr(s, 'updated_at') and s.updated_at else created
                project_tag = f"[{s.project}]" if hasattr(s, 'project') and s.project else "[default]"
                source_url = s.source_url if hasattr(s, 'source_url') and s.source_url else None

                print(f"  ‚Ä¢ {s.name} {project_tag} (Active: {s.active}) - Created: {created}, Updated: {updated}")
                if source_url:
                    print(f"    Source: {source_url}")
        else:
            if hasattr(args, 'project') and args.project:
                print(f"No spiders found in project '{args.project}'.")
            else:
                print("No spiders found in database.")
            
    elif args.spiders_command == 'import':
        try:
            if args.file == '-':
                # Read from stdin
                import sys
                data = json.load(sys.stdin)
            else:
                # Read from file
                with open(args.file, 'r') as f:
                    data = json.load(f)
            
            # Get project name from args
            project_name = args.project if hasattr(args, 'project') else 'default'

            # Validate source_url is provided
            if 'source_url' not in data or not data['source_url']:
                print(f"‚ùå Error: 'source_url' is required in spider JSON")
                print(f"   Add the original website URL: \"source_url\": \"https://example.com\"")
                return

            # Check if spider exists
            existing = db.query(Spider).filter(Spider.name == data['name']).first()
            if existing:
                print(f"‚ö†Ô∏è  Spider '{data['name']}' already exists. Updating...")
                # Simple update: delete and recreate (or just update fields)
                # For now, let's just update the main fields and replace rules
                existing.allowed_domains = data['allowed_domains']
                existing.start_urls = data['start_urls']
                existing.source_url = data.get('source_url')  # Optional field
                existing.project = project_name

                # Clear old rules and settings
                db.query(SpiderRule).filter(SpiderRule.spider_id == existing.id).delete()
                db.query(SpiderSetting).filter(SpiderSetting.spider_id == existing.id).delete()
                spider = existing
            else:
                spider = Spider(
                    name=data['name'],
                    allowed_domains=data['allowed_domains'],
                    start_urls=data['start_urls'],
                    source_url=data.get('source_url'),  # Optional field
                    project=project_name
                )
                db.add(spider)
                db.flush() # Get ID
            
            # Add Rules
            for r_data in data.get('rules', []):
                rule = SpiderRule(
                    spider_id=spider.id,
                    allow_patterns=r_data.get('allow'),
                    deny_patterns=r_data.get('deny'),
                    restrict_xpaths=r_data.get('restrict_xpaths'),
                    restrict_css=r_data.get('restrict_css'),
                    callback=r_data.get('callback'),
                    follow=r_data.get('follow', True),
                    priority=r_data.get('priority', 0)
                )
                db.add(rule)
            
            # Add Settings
            for k, v in data.get('settings', {}).items():
                setting = SpiderSetting(
                    spider_id=spider.id,
                    key=k,
                    value=str(v),
                    type=type(v).__name__
                )
                db.add(setting)
            
            db.commit()
            print(f"‚úÖ Spider '{spider.name}' imported successfully!")
            
        except Exception as e:
            db.rollback()
            print(f"‚ùå Error importing spider: {e}")
            
    elif args.spiders_command == 'delete':
        query = db.query(Spider).filter(Spider.name == args.name)

        # Filter by project if specified
        if hasattr(args, 'project') and args.project:
            query = query.filter(Spider.project == args.project)
            project_msg = f" in project '{args.project}'"
        else:
            project_msg = ""

        spider = query.first()

        if spider:
            if not args.force:
                confirm = input(f"Are you sure you want to delete spider '{args.name}'{project_msg}? (y/N): ")
                if confirm.lower() != 'y':
                    print("‚ùå Delete cancelled")
                    return

            db.delete(spider)
            db.commit()
            print(f"üóëÔ∏è  Spider '{args.name}'{project_msg} deleted!")
        else:
            print(f"‚ùå Spider '{args.name}'{project_msg} not found.")

def handle_projects_command(args):
    """Handle project management commands"""
    from core.db import get_db
    from core.models import Spider, CrawlQueue
    from sqlalchemy import func, distinct

    db = next(get_db())

    if args.projects_command == 'list':
        # Get distinct projects from both spiders and queue
        spider_projects = db.query(distinct(Spider.project)).filter(Spider.project.isnot(None)).all()
        queue_projects = db.query(distinct(CrawlQueue.project_name)).filter(CrawlQueue.project_name.isnot(None)).all()

        # Combine and deduplicate
        all_projects = set()
        for (proj,) in spider_projects:
            all_projects.add(proj)
        for (proj,) in queue_projects:
            all_projects.add(proj)

        if all_projects:
            print("üìÅ Available Projects:")
            for proj in sorted(all_projects):
                # Get counts
                spider_count = db.query(func.count(Spider.id)).filter(Spider.project == proj).scalar()
                queue_count = db.query(func.count(CrawlQueue.id)).filter(CrawlQueue.project_name == proj).scalar()

                print(f"  ‚Ä¢ {proj}")
                print(f"    Spiders: {spider_count}, Queue items: {queue_count}")
        else:
            print("No projects found.")

def handle_queue_command(args):
    """Handle queue management commands"""
    from core.db import get_db
    from core.models import CrawlQueue
    from datetime import datetime
    import socket
    import getpass
    from sqlalchemy import text

    db = next(get_db())

    if args.queue_command == 'add':
        # Check if URL already exists in queue for this project
        existing = db.query(CrawlQueue).filter(
            CrawlQueue.project_name == args.project,
            CrawlQueue.website_url == args.url
        ).first()

        if existing:
            status_emoji = {
                'pending': '‚è≥',
                'processing': 'üîÑ',
                'completed': '‚úÖ',
                'failed': '‚ùå'
            }.get(existing.status, '‚ùì')

            print(f"‚ö†Ô∏è  URL already exists in queue")
            print(f"   {status_emoji} ID: {existing.id}")
            print(f"   Status: {existing.status}")
            print(f"   URL: {existing.website_url}")
            print(f"   Skipping duplicate...")
            return

        # Add new item to queue
        queue_item = CrawlQueue(
            project_name=args.project,
            website_url=args.url,
            custom_instruction=args.custom_instruction,
            priority=args.priority
        )
        db.add(queue_item)
        db.commit()

        print(f"‚úÖ Added to queue (ID: {queue_item.id})")
        print(f"   URL: {args.url}")
        print(f"   Project: {args.project}")
        print(f"   Priority: {args.priority}")
        if args.custom_instruction:
            print(f"   Instructions: {args.custom_instruction}")

    elif args.queue_command == 'list':
        # List queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.status:
            # If status is explicitly specified, filter by that status
            query = query.filter(CrawlQueue.status == args.status)
        elif not args.all:
            # By default, exclude failed and completed items (show only pending and processing)
            query = query.filter(CrawlQueue.status.in_(['pending', 'processing']))

        # If --count flag is set, just return the count
        if args.count:
            count = query.count()
            status_msg = f" ({args.status})" if args.status else ""
            print(f"{count}")
            return

        query = query.order_by(CrawlQueue.priority.desc(), CrawlQueue.created_at.asc())

        if args.limit:
            query = query.limit(args.limit)

        items = query.all()

        if not items:
            status_msg = f" with status '{args.status}'" if args.status else ""
            print(f"üìã No items in queue for project '{args.project}'{status_msg}")
            return

        print(f"üìã Queue for project '{args.project}':")
        print()

        for item in items:
            status_emoji = {
                'pending': '‚è≥',
                'processing': 'üîÑ',
                'completed': '‚úÖ',
                'failed': '‚ùå'
            }.get(item.status, '‚ùì')

            print(f"{status_emoji} [{item.id}] {item.website_url}")
            print(f"   Status: {item.status} | Priority: {item.priority}")

            if item.custom_instruction:
                print(f"   Instructions: {item.custom_instruction}")

            if item.processing_by:
                locked_time = item.locked_at.strftime('%Y-%m-%d %H:%M') if item.locked_at else 'Unknown'
                print(f"   Processing by: {item.processing_by} (since {locked_time})")

            if item.error_message:
                print(f"   Error: {item.error_message}")

            if item.completed_at:
                completed_time = item.completed_at.strftime('%Y-%m-%d %H:%M')
                print(f"   Completed: {completed_time}")

            print()

    elif args.queue_command == 'next':
        # Atomic claim next item (supports both PostgreSQL and SQLite)
        from core.db import is_postgres
        processing_by = f"{getpass.getuser()}@{socket.gethostname()}"

        if is_postgres():
            # PostgreSQL: Use FOR UPDATE SKIP LOCKED for best performance
            result = db.execute(text("""
                UPDATE crawl_queue
                SET
                    status = 'processing',
                    processing_by = :processing_by,
                    locked_at = NOW(),
                    updated_at = NOW()
                WHERE id = (
                    SELECT id
                    FROM crawl_queue
                    WHERE status = 'pending'
                      AND project_name = :project_name
                    ORDER BY priority DESC, created_at ASC
                    LIMIT 1
                    FOR UPDATE SKIP LOCKED
                )
                RETURNING id, website_url, custom_instruction, priority
            """), {
                'processing_by': processing_by,
                'project_name': args.project
            })
        else:
            # SQLite: Use optimistic locking (re-check status in WHERE clause)
            result = db.execute(text("""
                UPDATE crawl_queue
                SET
                    status = 'processing',
                    processing_by = :processing_by,
                    locked_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = (
                    SELECT id
                    FROM crawl_queue
                    WHERE status = 'pending'
                      AND project_name = :project_name
                    ORDER BY priority DESC, created_at ASC
                    LIMIT 1
                )
                AND status = 'pending'
                RETURNING id, website_url, custom_instruction, priority
            """), {
                'processing_by': processing_by,
                'project_name': args.project
            })

        # Fetch result BEFORE committing (SQLite requires this)
        row = result.fetchone()
        db.commit()

        if row:
            print(f"üîÑ Claimed item from queue:")
            print(f"   ID: {row[0]}")
            print(f"   URL: {row[1]}")
            if row[2]:
                print(f"   Instructions: {row[2]}")
            print(f"   Priority: {row[3]}")
            print(f"   Locked by: {processing_by}")
        else:
            print(f"üì≠ No pending items in queue for project '{args.project}'")

    elif args.queue_command == 'complete':
        # Mark item as completed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        now = datetime.now(timezone.utc)
        item.status = 'completed'
        item.completed_at = now
        item.updated_at = now
        db.commit()

        print(f"‚úÖ Item {args.id} marked as completed")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'fail':
        # Mark item as failed
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'failed'
        item.error_message = args.error_message
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"‚ùå Item {args.id} marked as failed")
        print(f"   URL: {item.website_url}")
        if args.error_message:
            print(f"   Error: {args.error_message}")

    elif args.queue_command == 'retry':
        # Retry a failed item
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        from datetime import timezone
        item.status = 'pending'
        item.retry_count += 1
        item.error_message = None
        item.processing_by = None
        item.locked_at = None
        item.updated_at = datetime.now(timezone.utc)
        db.commit()

        print(f"üîÑ Item {args.id} reset to pending (retry count: {item.retry_count})")
        print(f"   URL: {item.website_url}")

    elif args.queue_command == 'remove':
        # Remove item from queue
        item = db.query(CrawlQueue).filter(CrawlQueue.id == args.id).first()

        if not item:
            print(f"‚ùå Queue item {args.id} not found")
            return

        url = item.website_url
        db.delete(item)
        db.commit()

        print(f"üóëÔ∏è  Item {args.id} removed from queue")
        print(f"   URL: {url}")

    elif args.queue_command == 'cleanup':
        # Bulk cleanup queue items
        query = db.query(CrawlQueue).filter(CrawlQueue.project_name == args.project)

        if args.all:
            # Remove all completed and failed items
            query = query.filter(CrawlQueue.status.in_(['completed', 'failed']))
        elif args.completed:
            # Remove only completed items
            query = query.filter(CrawlQueue.status == 'completed')
        elif args.failed:
            # Remove only failed items
            query = query.filter(CrawlQueue.status == 'failed')
        else:
            print("‚ùå Please specify --completed, --failed, or --all")
            return

        items = query.all()

        if not items:
            status_filter = "all completed and failed" if args.all else ("completed" if args.completed else "failed")
            print(f"üìã No {status_filter} items to cleanup in project '{args.project}'")
            return

        # Show what will be deleted
        print(f"üóëÔ∏è  Found {len(items)} items to remove:")
        for item in items[:5]:  # Show first 5
            status_emoji = '‚úÖ' if item.status == 'completed' else '‚ùå'
            print(f"   {status_emoji} [{item.id}] {item.website_url}")
        if len(items) > 5:
            print(f"   ... and {len(items) - 5} more")

        # Confirm unless --force
        if not args.force:
            confirm = input(f"\nRemove {len(items)} items? (y/N): ")
            if confirm.lower() != 'y':
                print("‚ùå Cleanup cancelled")
                return

        # Delete items
        for item in items:
            db.delete(item)
        db.commit()

        print(f"‚úÖ Removed {len(items)} items from queue")

    elif args.queue_command == 'bulk':
        # Bulk add URLs from JSON or CSV file
        import json
        import csv
        from pathlib import Path

        file_path = Path(args.file)

        try:
            # Detect file format by extension
            if file_path.suffix.lower() == '.csv':
                # Parse CSV file
                with open(args.file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    data = list(reader)

                    # Validate CSV has 'url' column
                    if not data or 'url' not in data[0]:
                        print("‚ùå CSV file must have a 'url' column")
                        return

            elif file_path.suffix.lower() == '.json':
                # Parse JSON file
                with open(args.file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if not isinstance(data, list):
                    print("‚ùå JSON file must contain an array of objects")
                    return
            else:
                print(f"‚ùå Unsupported file format: {file_path.suffix}")
                print("   Supported formats: .json, .csv")
                return

        except FileNotFoundError:
            print(f"‚ùå File not found: {args.file}")
            return
        except json.JSONDecodeError as e:
            print(f"‚ùå Invalid JSON: {e}")
            return
        except csv.Error as e:
            print(f"‚ùå Invalid CSV: {e}")
            return
        except Exception as e:
            print(f"‚ùå Error reading file: {e}")
            return

        added = 0
        skipped = 0

        for item in data:
            url = item.get('url')
            if not url:
                print(f"‚ö†Ô∏è  Skipping item without URL: {item}")
                skipped += 1
                continue

            # Check if URL already exists in queue for this project
            existing = db.query(CrawlQueue).filter(
                CrawlQueue.project_name == args.project,
                CrawlQueue.website_url == url
            ).first()

            if existing:
                skipped += 1
                continue

            # Get custom instruction from 'name' or 'custom_instruction' field
            custom_instruction = item.get('custom_instruction') or item.get('name')

            # Get priority from item or use default
            item_priority = item.get('priority')
            if item_priority is not None:
                try:
                    item_priority = int(item_priority)
                except (ValueError, TypeError):
                    item_priority = args.priority
            else:
                item_priority = args.priority

            # Add new item to queue
            queue_item = CrawlQueue(
                project_name=args.project,
                website_url=url,
                custom_instruction=custom_instruction,
                priority=item_priority
            )
            db.add(queue_item)
            added += 1

        db.commit()

        print(f"‚úÖ Bulk add complete:")
        print(f"   Added: {added}")
        print(f"   Skipped (duplicates/invalid): {skipped}")
        print(f"   Project: {args.project}")
        print(f"   Format: {file_path.suffix.upper()}")

def handle_db_command(args):
    """Handle database management commands"""
    import subprocess
    import sys
    import os
    
    if args.db_command == 'migrate':
        print("üîÑ Running database migrations...")
        try:
            result = subprocess.run([
                sys.executable, '-m', 'alembic', 'upgrade', 'head'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
            if result.returncode == 0:
                print("‚úÖ Migrations completed successfully!")
            else:
                print("‚ùå Migration failed!")
        except Exception as e:
            print(f"‚ùå Error running migrations: {e}")
            
    elif args.db_command == 'current':
        try:
            subprocess.run([
                sys.executable, '-m', 'alembic', 'current'
            ], cwd=os.path.dirname(os.path.abspath(__file__)))
        except Exception as e:
            print(f"‚ùå Error checking current revision: {e}")

def handle_show_command(args):
    """Show scraped articles from database"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from datetime import datetime

    db = next(get_db())

    # Find spider by name
    query = db.query(Spider).filter(Spider.name == args.spider_name)

    # Filter by project if specified
    if hasattr(args, 'project') and args.project:
        query = query.filter(Spider.project == args.project)
        project_msg = f" in project '{args.project}'"
    else:
        project_msg = ""

    spider = query.first()
    if not spider:
        print(f"‚ùå Spider '{args.spider_name}'{project_msg} not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()

    if not items:
        print(f"üì≠ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    print(f"üì∞ Showing {len(items)} articles from '{args.spider_name}':")
    if filters_applied:
        print(f"   (filtered by: {', '.join(filters_applied)})")
    print()

    for i, item in enumerate(items, 1):
        # Format date nicely
        scraped_date = item.scraped_at.strftime('%Y-%m-%d %H:%M') if item.scraped_at else 'Unknown'
        pub_date = item.published_date.strftime('%Y-%m-%d') if item.published_date else 'Unknown'

        print(f"üî∏ [{i}] {item.title or 'No Title'}")
        print(f"   üìÖ Published: {pub_date} | Scraped: {scraped_date}")
        print(f"   üîó {item.url}")
        if item.author:
            print(f"   ‚úçÔ∏è  {item.author}")

        # Show content preview (first 150 chars)
        if item.content:
            content_preview = item.content[:150].replace('\n', ' ').strip()
            if len(item.content) > 150:
                content_preview += "..."
            print(f"   üìù {content_preview}")
        print()

def handle_export_command(args):
    """Export scraped articles from database to various formats"""
    from core.db import get_db
    from core.models import Spider, ScrapedItem
    from pathlib import Path
    import csv
    import json

    db = next(get_db())

    # Find spider by name
    query = db.query(Spider).filter(Spider.name == args.spider_name)

    # Filter by project if specified
    if hasattr(args, 'project') and args.project:
        query = query.filter(Spider.project == args.project)
        project_msg = f" in project '{args.project}'"
    else:
        project_msg = ""

    spider = query.first()
    if not spider:
        print(f"‚ùå Spider '{args.spider_name}'{project_msg} not found in database.")
        return

    # Build query
    query = db.query(ScrapedItem).filter(ScrapedItem.spider_id == spider.id)

    # Apply filters
    filters_applied = []
    if args.url:
        query = query.filter(ScrapedItem.url.ilike(f'%{args.url}%'))
        filters_applied.append(f"URL contains '{args.url}'")

    if args.title:
        query = query.filter(ScrapedItem.title.ilike(f'%{args.title}%'))
        filters_applied.append(f"title contains '{args.title}'")

    if args.text:
        from sqlalchemy import or_
        text_filter = or_(
            ScrapedItem.title.ilike(f'%{args.text}%'),
            ScrapedItem.content.ilike(f'%{args.text}%')
        )
        query = query.filter(text_filter)
        filters_applied.append(f"title or content contains '{args.text}'")

    # Get items ordered by most recent first
    if args.limit:
        items = query.order_by(ScrapedItem.scraped_at.desc()).limit(args.limit).all()
    else:
        items = query.order_by(ScrapedItem.scraped_at.desc()).all()

    if not items:
        print(f"üì≠ No articles found for spider '{args.spider_name}'")
        if filters_applied:
            print(f"   (with filters: {', '.join(filters_applied)})")
        return

    # Determine output file path
    if args.output:
        output_path = Path(args.output)
    else:
        # Create data directory if it doesn't exist
        from datetime import datetime
        data_dir = Path('data')
        data_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%d%m%Y_%H%M%S')
        output_path = data_dir / f"{args.spider_name}_export_{timestamp}.{args.format}"

    # Ensure parent directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert items to list of dicts
    items_data = []
    for item in items:
        item_dict = {
            'id': item.id,
            'url': item.url,
            'title': item.title,
            'content': item.content,
            'author': item.author,
            'published_date': item.published_date.isoformat() if item.published_date else None,
            'scraped_at': item.scraped_at.isoformat() if item.scraped_at else None,
            'metadata': item.metadata_json
        }
        items_data.append(item_dict)

    try:
        # Export based on format
        if args.format == 'csv':
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                if items_data:
                    writer = csv.DictWriter(f, fieldnames=items_data[0].keys())
                    writer.writeheader()
                    writer.writerows(items_data)
            print(f"‚úÖ Exported {len(items_data)} articles to CSV: {output_path}")

        elif args.format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(items_data, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ Exported {len(items_data)} articles to JSON: {output_path}")

        elif args.format == 'jsonl':
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in items_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"‚úÖ Exported {len(items_data)} articles to JSONL: {output_path}")

        elif args.format == 'parquet':
            try:
                import pandas as pd
                df = pd.DataFrame(items_data)
                df.to_parquet(output_path, index=False)
                print(f"‚úÖ Exported {len(items_data)} articles to Parquet: {output_path}")
            except ImportError:
                print("‚ùå Parquet export requires pandas and pyarrow libraries.")
                print("   Run: pip install pandas pyarrow")
                return

        if filters_applied:
            print(f"   (filtered by: {', '.join(filters_applied)})")

    except Exception as e:
        print(f"‚ùå Error exporting data: {e}")

def handle_extract_urls_command(args):
    """Extract all URLs from an HTML file"""
    from utils.url_extractor import extract_urls_from_html

    try:
        urls = extract_urls_from_html(args.file, args.output)

        # If no output file specified, print to stdout
        if not args.output:
            print('\n'.join(urls))
        else:
            print(f"‚úÖ Extracted {len(urls)} URLs to {args.output}")

    except FileNotFoundError as e:
        print(f"‚ùå {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

def run_spider(project_name, spider_name, output_file=None, limit=None, timeout=None):
    """Run a Scrapy spider (DB or File-based)"""
    # Try to find spider in DB first
    from core.db import get_db
    from core.models import Spider

    db = next(get_db())
    db_spider = db.query(Spider).filter(Spider.name == spider_name).first()

    if db_spider:
        print(f"üöÄ Running DB spider: {spider_name}")
        import sys
        from datetime import datetime

        # Check if spider uses Cloudflare bypass
        cf_enabled = False
        if db_spider.settings:
            for setting in db_spider.settings:
                if setting.key == "CLOUDFLARE_ENABLED" and str(setting.value).lower() in ["true", "1"]:
                    cf_enabled = True
                    break

        cmd = [sys.executable, '-m', 'scrapy', 'crawl', 'database_spider', '-a', f'spider_name={spider_name}']

        # Smart storage logic based on --limit flag
        if limit:
            # Testing mode: Save to database for verification with `show` command
            print(f"üß™ Test mode: Saving to database (limit: {limit} items)")
            print(f"   Use './scrapai show {spider_name}' to verify results")
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
            # Don't include HTML in test mode (saves DB storage)
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=False'])
        else:
            # Production mode: Export to files, disable database to save costs
            print(f"üìÅ Production mode: Exporting to files (database disabled)")

            # Disable DatabasePipeline to avoid DB costs
            cmd.extend(['-s', 'ITEM_PIPELINES={"pipelines.ScrapaiPipeline": 300}'])

            # Include HTML in JSONL output for full scraping
            cmd.extend(['-s', 'INCLUDE_HTML_IN_OUTPUT=True'])

            # Add default output file if not specified
            if not output_file:
                import os
                now = datetime.now()
                date_folder = now.strftime('%Y-%m-%d')
                output_dir = f'data/{spider_name}/{date_folder}'
                os.makedirs(output_dir, exist_ok=True)
                timestamp = now.strftime('%H%M%S')
                output_file = f'{output_dir}/crawl_{timestamp}.jsonl'

            cmd.extend(['-o', output_file])
            print(f"   Output: {output_file} (includes HTML)")

        # Add explicit output file if provided
        if output_file and limit:
            cmd.extend(['-o', output_file])
            print(f"   Also saving to: {output_file}")

        # Add graceful timeout if specified (CLOSESPIDER_TIMEOUT)
        if timeout:
            cmd.extend(['-s', f'CLOSESPIDER_TIMEOUT={timeout}'])
            hours = timeout / 3600
            print(f"‚è±Ô∏è  Max runtime: {hours:.1f} hours (graceful stop)")

        # Auto-detect if xvfb is needed for Cloudflare bypass
        if cf_enabled:
            from utils.display_helper import needs_xvfb, has_xvfb

            if needs_xvfb():
                if has_xvfb():
                    print("üñ•Ô∏è  Headless environment detected - using xvfb for Cloudflare bypass")
                    cmd = ['xvfb-run', '-a'] + cmd
                else:
                    print("‚ö†Ô∏è  WARNING: Cloudflare bypass enabled but no display available and xvfb not installed")
                    print("   Install xvfb: sudo apt-get install xvfb")
                    print("   Continuing anyway - browser may fail to start...")
            else:
                print("üñ•Ô∏è  Display available - using native browser for Cloudflare bypass")

        subprocess.run(cmd)
        return

    # Fallback to legacy file-based project system
    if not project_name:
        print(f"‚ùå Spider '{spider_name}' not found in DB. To run legacy file-based spider, please provide --project.")
        return

    config_loader = ConfigLoader()
    
    try:
        # Validate project and spider
        if not config_loader.validate_spider_for_project(project_name, spider_name):
            print(f"‚ùå Spider '{spider_name}' not found in DB and not configured for project '{project_name}'")
            return
        
        # Get project settings
        settings = config_loader.get_spider_settings(project_name, spider_name)
        
        # Build scrapy command
        import sys
        cmd = [sys.executable, '-m', 'scrapy', 'crawl', spider_name]
        
        # Add settings (only simple ones that can be passed via command line)
        for key, value in settings.items():
            if key not in ['FEEDS', 'LOG_FILE', 'ITEM_PIPELINES']:  # These are handled differently
                # Only add simple settings that can be passed via command line
                if isinstance(value, (str, int, float, bool)):
                    cmd.extend(['-s', f'{key}={value}'])
        
        # Handle output
        if output_file:
            cmd.extend(['-o', output_file])
        else:
            # Create output directory and add default output
            import os
            output_dir = f'projects/{project_name}/outputs/{spider_name}'
            os.makedirs(output_dir, exist_ok=True)
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'{output_dir}/{timestamp}.json'
            cmd.extend(['-o', output_file])
        
        # Handle log file
        if 'LOG_FILE' in settings:
            cmd.extend(['-s', f'LOG_FILE={settings["LOG_FILE"]}'])
        
        if limit:
            cmd.extend(['-s', f'CLOSESPIDER_ITEMCOUNT={limit}'])
        
        print(f"üöÄ Running spider: {spider_name} (project: {project_name})")
        subprocess.run(cmd)
        
    except Exception as e:
        print(f"‚ùå Error running spider: {e}")

def run_all_spiders(project_name, limit=None):
    """Run all spiders configured for a project"""
    config_loader = ConfigLoader()
    
    try:
        spiders = config_loader.get_project_spiders(project_name)
        if not spiders:
            print(f"‚ùå No spiders configured for project '{project_name}'")
            return
        
        print(f"üöÄ Running all spiders for project: {project_name}")
        print(f"üï∑Ô∏è  Spiders: {', '.join(spiders)}")
        
        for spider in spiders:
            print(f"\n{'='*50}")
            print(f"Running: {spider}")
            print(f"{'='*50}")
            run_spider(project_name, spider, None, limit)
            
    except Exception as e:
        print(f"‚ùå Error running spiders: {e}")

if __name__ == '__main__':
    main()