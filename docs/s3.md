# S3 Object Storage

ScrapAI can automatically upload crawl results to S3-compatible object storage for backup, archiving, and sharing across systems.

## Supported Providers

Works with any S3-compatible object storage:

- **Hetzner Object Storage** ⭐ **(We use this)** - European provider, excellent pricing, free egress
- **AWS S3** - Amazon's original object storage
- **DigitalOcean Spaces** - Simple, developer-friendly
- **Wasabi** - Hot cloud storage, cheaper than S3
- **Backblaze B2** - Cost-effective alternative
- **Cloudflare R2** - Zero egress fees
- **MinIO** - Self-hosted S3-compatible storage
- Any other S3-compatible provider

## Setup

Add S3 credentials to `.env`:

```bash
# S3-Compatible Object Storage
# Uncomment and fill in your S3 details to enable automatic uploads
# S3_ACCESS_KEY=your_access_key_here
# S3_SECRET_KEY=your_secret_key_here
# S3_ENDPOINT=https://hel1.your-objectstorage.com
# S3_BUCKET=your-bucket-name
```

Uncomment the lines and fill in your provider credentials.

### Provider-Specific Endpoints

**Hetzner Object Storage** ⭐ **(Recommended - we use this)**:
```bash
# Get your endpoint from Hetzner Cloud Console → Object Storage
S3_ENDPOINT=https://fsn1.your-objectstorage.com  # Falkenstein, Germany
# or
S3_ENDPOINT=https://hel1.your-objectstorage.com  # Helsinki, Finland
# or
S3_ENDPOINT=https://nbg1.your-objectstorage.com  # Nuremberg, Germany

# Full example for Hetzner:
S3_ACCESS_KEY=your_hetzner_access_key
S3_SECRET_KEY=your_hetzner_secret_key
S3_ENDPOINT=https://fsn1.your-objectstorage.com
S3_BUCKET=scrapai-crawls
```

**DigitalOcean Spaces:**
```bash
S3_ENDPOINT=https://nyc3.digitaloceanspaces.com  # New York
# or
S3_ENDPOINT=https://sfo3.digitaloceanspaces.com  # San Francisco
```

**Wasabi:**
```bash
S3_ENDPOINT=https://s3.us-east-1.wasabisys.com   # US East
# or
S3_ENDPOINT=https://s3.eu-central-1.wasabisys.com # EU Central
```

**AWS S3:**
```bash
S3_ENDPOINT=https://s3.us-east-1.amazonaws.com   # US East
# Note: AWS S3 endpoints vary by region
```

**Backblaze B2:**
```bash
S3_ENDPOINT=https://s3.us-west-000.backblazeb2.com  # Check your B2 dashboard for correct endpoint
```

**Cloudflare R2:**
```bash
S3_ENDPOINT=https://<account_id>.r2.cloudflarestorage.com
```

## How It Works

**Current Implementation:**
- S3 upload is **only supported in Airflow workflows**
- Regular `./scrapai crawl` commands do **not** upload to S3 (yet)
- Airflow DAGs automatically detect S3 configuration
- Upload happens in background after spider completes
- Does not block crawling or slow down scraping

**Upload Process:**
1. Spider completes crawl
2. Crawl results saved to `DATA_DIR/<project>/<spider>/crawls/crawl_TIMESTAMP.jsonl`
3. If S3 configured, Airflow task uploads file to S3 bucket
4. Files remain locally even after upload
5. S3 path: `s3://<bucket>/<spider_name>/crawl_TIMESTAMP.jsonl`

**Local Files:**
All crawl data remains in your local `DATA_DIR` even after S3 upload. S3 is for backup/archiving, not a replacement for local storage.

## Configuration Check

Airflow DAGs log S3 status on startup:
```
S3 Upload: ENABLED
```
or
```
S3 Upload: DISABLED (credentials not found)
```

If disabled, check that all 4 environment variables are set in `.env`:
- `S3_ACCESS_KEY`
- `S3_SECRET_KEY`
- `S3_ENDPOINT`
- `S3_BUCKET`

## When to Mention to Users

Recommend S3 setup when:
- User asks about data backup or archiving
- User needs to share crawl data across systems
- User mentions cloud storage (S3, Hetzner, DigitalOcean Spaces, etc.)
- User has large crawls and wants off-site storage
- User is using Airflow workflows
- User asks about data durability/redundancy

## Use Cases

**Backup & Disaster Recovery:**
- Automatic off-site backup of all crawl data
- Protect against local disk failure
- Easy restoration from S3 if needed

**Data Sharing:**
- Share crawl results across multiple systems
- Centralized data storage for team access
- Export data from one system, import on another

**Archiving:**
- Long-term storage of historical crawls
- Cheaper than keeping on local SSD/NVMe
- Compliance and audit requirements

**Multi-Region Deployment:**
- Run crawlers in multiple regions
- All data centralized in S3 bucket
- Access from anywhere

## Limitations

**Current Limitations:**
- ⚠️ **Only works with Airflow workflows** - Regular `./scrapai crawl` does not upload to S3
- Upload happens after entire crawl completes (not streaming)
- No selective upload (all crawls uploaded if S3 enabled)
- No automatic cleanup of old files in S3

**Workaround for non-Airflow crawls:**
If you need S3 upload for regular `./scrapai crawl` commands, manually upload after crawl:
```bash
# After crawl completes
aws s3 cp DATA_DIR/<project>/<spider>/crawls/crawl_20260223_143022.jsonl \
  s3://your-bucket/<spider_name>/
```

## Cost Considerations

**Storage Costs:**
- Hetzner: ~€0.005/GB/month (~$0.005)
- DigitalOcean Spaces: $0.02/GB/month (first 250GB free with droplet)
- Wasabi: $0.0059/GB/month (minimum 1TB)
- AWS S3: $0.023/GB/month (US East)
- Backblaze B2: $0.005/GB/month
- Cloudflare R2: $0.015/GB/month

**Egress Costs (downloading from S3):**
- Hetzner: Free egress
- DigitalOcean Spaces: Free up to 1TB/month
- Wasabi: Free egress
- AWS S3: $0.09/GB (expensive!)
- Backblaze B2: $0.01/GB
- Cloudflare R2: **Zero egress fees** (best for frequent downloads)

**Tip:** For frequent data downloads, use providers with free/cheap egress (Cloudflare R2, Hetzner, Wasabi).

## Troubleshooting

### S3 upload not happening
1. Check `.env` has all 4 S3 variables set
2. Verify credentials are correct (test with AWS CLI or s3cmd)
3. Confirm bucket exists and is accessible
4. Check Airflow DAG logs for errors
5. Regular `./scrapai crawl` does NOT upload to S3 (only Airflow workflows)

### Upload fails with authentication error
1. Double-check access key and secret key
2. Verify endpoint URL is correct for your provider
3. Ensure bucket exists in the same region as endpoint
4. Check IAM permissions (AWS) or access policies (other providers)

### Files not appearing in S3 bucket
1. Check bucket name is correct
2. Verify upload task completed successfully in Airflow logs
3. Browse S3 bucket with web console or CLI
4. Check S3 path: `s3://<bucket>/<spider_name>/crawl_*.jsonl`

### Upload too slow
1. Check network bandwidth to S3 provider
2. Consider provider closer to your location
3. Split large crawls into smaller batches
4. Use provider with faster upload speeds

## Future Enhancements

Planned features:
- [ ] S3 upload for regular `./scrapai crawl` (not just Airflow)
- [ ] Streaming upload during crawl (not just at end)
- [ ] Selective upload (upload only specific spiders)
- [ ] Automatic cleanup of old S3 files
- [ ] Compression before upload (gzip)
- [ ] Upload progress reporting
- [ ] S3 download command (restore from S3)
- [ ] Multi-bucket support (different buckets per project)

## See Also

- [Queue Management](queue.md) - Batch processing with Airflow
- [Analysis Workflow](analysis-workflow.md) - Spider building workflow
- [Onboarding Guide](onboarding.md) - Initial setup
