# ScrapAI Configuration
# Copy this file to .env and customize as needed

# Data directory - where all scraped data, analysis, and artifacts are stored
DATA_DIR=./data

# Database connection string
# Default: SQLite (no setup required, perfect for getting started)
# For production/scale: use PostgreSQL (postgresql://user:password@localhost:5432/scrapai)
DATABASE_URL=sqlite:///scrapai.db

# Proxy Configuration (SmartProxyMiddleware)
# ScrapAI automatically uses proxies when needed (on 403/429 errors) and learns which domains require them
# Uncomment and fill in your proxy details to enable

# Datacenter Proxy (recommended for most use cases - faster, cheaper)
# DATACENTER_PROXY_USERNAME=your_username
# DATACENTER_PROXY_PASSWORD=your_password
# DATACENTER_PROXY_HOST=your-datacenter-proxy.com
# DATACENTER_PROXY_PORT=10000  # Port 10000 = rotating IPs (recommended)

# Residential Proxy (used with --proxy-type residential flag)
# Use for sites that block datacenter IPs
# RESIDENTIAL_PROXY_USERNAME=your_username
# RESIDENTIAL_PROXY_PASSWORD=your_password
# RESIDENTIAL_PROXY_HOST=your-residential-proxy.com
# RESIDENTIAL_PROXY_PORT=7000  # Port 7000 = rotating residential IPs

# Logging
# LOG_LEVEL=info
# LOG_DIR=./logs

# Airflow Configuration (for docker-compose.airflow.yml)
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin123
AIRFLOW_UID=501

# S3-Compatible Object Storage (Hetzner, DigitalOcean Spaces, Wasabi, Backblaze, etc.)
# If these are set, scraped data will be automatically uploaded to S3 after crawling
# Uncomment and fill in your S3 details to enable automatic uploads
# S3_ACCESS_KEY=your_access_key_here
# S3_SECRET_KEY=your_secret_key_here
# S3_ENDPOINT=https://your-s3-endpoint.com
# S3_BUCKET=your-bucket-name
