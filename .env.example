# ScrapAI Configuration
# Copy this file to .env and customize as needed

# Data directory - where all scraped data, analysis, and artifacts are stored
DATA_DIR=./data

# Database connection string
# Default: SQLite (no setup required, perfect for getting started)
# For production/scale: use PostgreSQL (postgresql://user:password@localhost:5432/scrapai)
DATABASE_URL=sqlite:///scrapai.db

# Logging
# LOG_LEVEL=info
# LOG_DIR=./logs

# Airflow Configuration (for docker-compose.airflow.yml)
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=changeme123
AIRFLOW_UID=501

# S3-Compatible Object Storage (Hetzner, DigitalOcean Spaces, Wasabi, Backblaze, etc.)
# If these are set, scraped data will be automatically uploaded to S3 after crawling
# Leave empty to disable S3 upload and store only locally
S3_ACCESS_KEY=your_access_key_here
S3_SECRET_KEY=your_secret_key_here
S3_ENDPOINT=https://hel1.your-objectstorage.com  # Hetzner example, or your S3-compatible endpoint
S3_BUCKET=your-bucket-name
