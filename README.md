# scrapai-cli

Project-based Scrapy spider management for large-scale web scraping. Built for Claude Code to intelligently analyze and scrape websites with multi-project isolation and newspaper4k-powered content extraction.

## For Claude Code Instances

**When asked to add any website, follow the 3-phase enhanced workflow:**

1. **Phase 1: URL Collection** â†’ Create basic spider to collect article URLs
2. **Phase 2: Parser Creation** â†’ Analyze samples and create newspaper4k parser
3. **Phase 3: Integration** â†’ Integrate parser into spider and deploy

**IMPORTANT:**

- Use the newspaper4k library for content extraction (not CSS selectors)
- Always test parsers on sample URLs before integration
- **ALWAYS inspect collected URLs** and make spider restrictive to avoid non-article pages
- Create spiders in the shared `spiders/` directory and parsers in `parsers/`
- Use project system for client isolation and output organization

## Claude Code Decision Process - ENHANCED WORKFLOW

**When user says: "Add [website] to our system" or "Create project X and add [website] to it"**

```
PHASE 1: PROJECT SETUP & HTML ANALYSIS
â”‚
â”œâ”€ 1. CREATE PROJECT (if needed)
â”‚  â””â”€ ./scrapai projects create --name X --spiders ""
â”‚
â”œâ”€ 2. INSPECT WEBSITE STRUCTURE
â”‚  â”œâ”€ bin/inspector --url https://website.com/
â”‚  â”œâ”€ Analyze homepage HTML structure and link patterns
â”‚  â”œâ”€ Identify article URL patterns vs non-article pages
â”‚  â””â”€ Create analysis files in data/website/analysis/
â”‚
â”œâ”€ 3. CREATE BASIC URL-COLLECTING SPIDER
â”‚  â”œâ”€ Create spiders/website.py based on inspector findings
â”‚  â”œâ”€ Use restrictive rules targeting ONLY article URLs
â”‚  â”œâ”€ Focus ONLY on collecting article URLs (not content extraction)
â”‚  â”œâ”€ Use homepage crawling to find articles (sitemap only if user requests)
â”‚  â””â”€ Save URLs to data/website/urls.json
â”‚
â”œâ”€ 4. RUN URL COLLECTION
â”‚  â”œâ”€ ./scrapai test --project X website --limit 5
â”‚  â””â”€ Collect representative article URLs for analysis
â”‚
â”œâ”€ 5. INSPECT COLLECTED URLS & ITERATIVE FILTERING
â”‚  â”œâ”€ Review collected URLs in projects/X/outputs/spider/latest.json
â”‚  â”œâ”€ If navigation/category pages are found instead of actual articles:
â”‚  â”‚  â”œâ”€ Add specific deny patterns to spider rules for those nav/category URLs
â”‚  â”‚  â”œâ”€ Re-run URL collection: ./scrapai test --project X website --limit 5
â”‚  â”‚  â””â”€ Repeat until only actual article URLs are captured
â”‚  â”œâ”€ Ensure only articles are captured (no categories, static pages, etc.)
â”‚  â””â”€ Continue iterating until spider collects real article content
â”‚
PHASE 2: CONTENT ANALYSIS & PARSER CREATION
â”‚
â”œâ”€ 6. ANALYZE SAMPLE ARTICLES
â”‚  â”œâ”€ Take 3-5 URLs from collected data/website/urls.json
â”‚  â”œâ”€ Show Claude Code the actual article URLs for inspection
â”‚  â”œâ”€ Claude analyzes HTML structure and content patterns
â”‚  â””â”€ Identify extraction requirements (title, content, author, date, etc.)
â”‚
â”œâ”€ 7. USE SHARED NEWSPAPER4K PARSER
â”‚  â”œâ”€ Import and use utils.newspaper_parser.parse_article() in spider
â”‚  â”œâ”€ Shared parser handles proxies, retries, and standardized output automatically
â”‚  â”œâ”€ No need to create domain-specific parser files
â”‚  â”œâ”€ Custom metadata extraction only if shared parser misses site-specific data
â”‚  â””â”€ Focus on spider URL collection rules, not parsing logic
â”‚
â”œâ”€ 8. TEST PARSER ON SAMPLE URLS
â”‚  â”œâ”€ Test parser against 5-10 collected URLs (using existing framework)
â”‚  â”œâ”€ Validate extraction: title, content, authors, publish_date, images
â”‚  â”œâ”€ Check newspaper4k's automatic extraction quality
â”‚  â”œâ”€ Use existing CLI commands for testing (no custom scripts)
â”‚  â””â”€ Refine parser if needed for missing metadata
â”‚
PHASE 3: INTEGRATION & DEPLOYMENT
â”‚
â”œâ”€ 9. INTEGRATE SHARED PARSER INTO SPIDER
â”‚  â”œâ”€ Update spiders/website.py to use utils.newspaper_parser.parse_article()
â”‚  â”œâ”€ Replace URL collection logic with content parsing
â”‚  â”œâ”€ Import from utils.newspaper_parser import parse_article
â”‚  â””â”€ Maintain Scrapy framework for URL discovery
â”‚
â”œâ”€ 10. FINAL TESTING
â”‚  â”œâ”€ ./scrapai test --project X website --limit 5
â”‚  â”œâ”€ Verify complete article extraction (not just URLs)
â”‚  â””â”€ Ensure 100% success rate for content extraction
â”‚
â”œâ”€ 11. ADD TO PROJECT CONFIG
â”‚  â”œâ”€ Edit projects/X/config.yaml
â”‚  â”œâ”€ Add "website" to spiders list
â”‚  â””â”€ Configure any site-specific settings
â”‚
â””â”€ 12. READY FOR PRODUCTION
   â”œâ”€ ./scrapai crawl --project X website --limit 100
   â””â”€ Full article content extraction with reliable newspaper4k
```

## Why newspaper4k Over CSS Selectors?

**Problems with Traditional CSS Selectors:**

- Break when sites update HTML structure
- Miss content from varying layouts
- Extract wrong content (ads, navigation)
- Require manual selector maintenance
- Fragile and site-specific

**Benefits of newspaper4k:**

- Automatic content extraction using ML
- Handles layout variations automatically
- Extracts clean article text reliably
- Provides metadata (authors, dates, images)
- Minimal custom code needed
- Works across different site structures

## Quick Start

### 1. Setup Environment

```bash
# Activate virtual environment (ALWAYS use this in the repo)
source .venv/bin/activate  # Linux/Mac

# Install dependencies if not already installed
pip install newspaper4k

# Install Playwright browsers (required for inspector, if not already installed)
playwright install

# Check available projects
./scrapai projects list
```

### 2. Project Management

**Create a new project:**

```bash
./scrapai projects create --name client-team-a --spiders ""
```

**List all projects:**

```bash
./scrapai projects list
```

**Check project status:**

```bash
./scrapai projects status --project client-team-a
```

### 3. Example: Adding a New Website

**User request:** "Add https://example-news-site.com/ to our system"

**Phase 1 - URL Collection:**

```bash
# Check if project exists or create new one
./scrapai projects list
./scrapai projects create --name MyProject --spiders ""

# Claude creates spiders/example_news.py (URL collection only)
# Run URL collection
./scrapai test --project MyProject example_news --limit 5
```

**Phase 2 - Parser Creation:**

```bash
# Claude analyzes sample URLs and creates parsers/example_news.py
# Test parser on sample URLs
./scrapai test --project MyProject example_news --limit 5
```

**Phase 3 - Integration:**

```bash
# Claude integrates parser into spider
# Final production test
./scrapai crawl --project MyProject example_news --limit 100
```

## Project Structure

```
scrapai-cli/
â”œâ”€â”€ spiders/                            # Shared spider library
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_spider.py                  # Base class with common functionality
â”‚   â”œâ”€â”€ politifact.py                   # Domain-specific spiders
â”œâ”€â”€ parsers/                            # newspaper4k-based parsers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ politifact.py                   # Domain-specific parsers
â”œâ”€â”€ projects/                           # Project instances (client isolation)
â”‚   â”œâ”€â”€ client-team-a/
â”‚   â”‚   â”œâ”€â”€ config.yaml                 # Project configuration
â”‚   â”‚   â”œâ”€â”€ outputs/                    # Project-specific outputs
â”‚   â”‚   â”‚   â”œâ”€â”€ politifact/
â”‚   â”‚   â”‚   â””â”€â”€ desmog/
â”‚   â”‚   â””â”€â”€ logs/                       # Project-specific logs
â”‚   â”œâ”€â”€ client-team-b/
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â””â”€â”€ internal-research/
â”‚       â”œâ”€â”€ config.yaml
â”‚       â”œâ”€â”€ outputs/
â”‚       â””â”€â”€ logs/
â”œâ”€â”€ data/                               # URL collection and analysis
â”‚   â””â”€â”€ politifact/
â”‚       â”œâ”€â”€ urls.json
â”‚       â””â”€â”€ analysis/
â”œâ”€â”€ core/                               # Analysis and project management
â”‚   â”œâ”€â”€ project_manager.py              # Project creation and management
â”‚   â”œâ”€â”€ config_loader.py                # YAML configuration handling
â”‚   â”œâ”€â”€ sitemap.py                      # Sitemap discovery
â”‚   â””â”€â”€ analyzer.py                     # Site structure analysis
â”œâ”€â”€ utils/                              # Utilities (http, logging, etc.)
â”œâ”€â”€ bin/                               # Analysis tools
â”œâ”€â”€ settings.py                        # Default Scrapy configuration
â”œâ”€â”€ scrapai                           # Enhanced CLI with project support
â””â”€â”€ scrapy.cfg                        # Scrapy configuration
```

## Shared Parser Implementation

**utils/newspaper_parser.py** (already created):

The system now uses a shared newspaper4k parser with built-in proxy support, retry logic, and standardized output. Key features:

- **Automatic proxy handling**: Integrates with existing `utils/http.py` proxy infrastructure
- **Retry logic**: Built-in retry with exponential backoff for failed requests
- **Content cleaning**: Automatic cleaning of titles and content
- **Standardized output**: Consistent data structure across all sites
- **Multiple proxy types**: Supports 'none', 'static', 'residential', and 'auto' modes

```python
from utils.newspaper_parser import parse_article

# Simple usage - auto proxy selection
article_data = parse_article(url, source_name='desmog')

# Advanced usage with specific proxy type
from utils.newspaper_parser import NewspaperParser
parser = NewspaperParser(proxy_type='residential')
article_data = parser.parse_article(url, source_name='desmog')
```

**Benefits:**
- âœ… No duplicate parser code across domains
- âœ… Centralized proxy configuration and retry logic
- âœ… Consistent output format for all sites
- âœ… Easy to maintain and update parsing logic
- âœ… Built-in content cleaning and validation

**spiders/website.py (integrated with shared parser):**

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from .base_spider import BaseSpider
from utils.newspaper_parser import parse_article

class WebsiteSpider(BaseSpider):
    name = 'website'
    allowed_domains = ['website.com', 'www.website.com']
    start_urls = ['https://www.website.com/']
    
    # Define crawling rules - liberal approach with deny rules only
    rules = (
        # Follow all article links, only excluding static/non-article pages
        Rule(LinkExtractor(
            deny=r'/(about|contact|donate|privacy|terms|advertise|careers|subscribe|help|support|login|register)/?$'
        ), callback='parse_article', follow=True),
        # Follow pagination and navigation pages
        Rule(LinkExtractor(
            allow=r'/(page|category|tag|archive)/',
            deny=r'/(about|contact|donate|privacy|terms)/'
        ), follow=True),
    )

    def parse_article(self, response):
        """Parse article using shared newspaper4k parser"""
        # Use the shared newspaper4k parser with automatic proxy handling
        article_data = parse_article(response.url, source_name=self.name)
        
        if article_data:
            # Create enhanced item using newspaper4k extracted data
            item = self.create_item(response, **article_data)
            yield item
        else:
            # Fallback for failed parsing
            self.logger.warning(f"Failed to parse article: {response.url}")
            item = self.create_item(response, 
                url=response.url,
                title=response.css('title::text').get(),
                status='parse_failed'
            )
            yield item
```

## Project Configuration

Each project has a `config.yaml` file:

```yaml
project_name: "client-team-a"
spiders:
  - politifact

settings:
  download_delay: 2
  concurrent_requests: 4
  concurrent_requests_per_domain: 2
  robotstxt_obey: true
  # newspaper4k settings
  newspaper_config:
    memoize_articles: false
    fetch_images: true
output_format: json
```

## CLI Commands

### Project Management

```bash
# Create project with spiders
./scrapai projects create --name BrownU --spiders politifact,desmog

# Create project without spiders (add later)
./scrapai projects create --name ClientA --spiders ""

# List all projects
./scrapai projects list

# Project status
./scrapai projects status --project client-team-a

# Delete project (with confirmation)
./scrapai projects delete --project client-team-a
```

### Spider Operations

```bash
# List all available spiders
./scrapai list

# List spiders for specific project
./scrapai list --project client-team-a

# Run specific spider
./scrapai crawl --project client-team-a website --limit 100

# Run all project spiders
./scrapai crawl-all --project client-team-a

# Test spider
./scrapai test --project client-team-a website --limit 5
```

### Monitoring

```bash
# Project status
./scrapai status --project client-team-a

# View logs
./scrapai logs --project client-team-a
./scrapai logs --project client-team-a --spider website
```

## Output Format

```json
{
  "url": "https://www.example-news-site.com/2025/07/08/sample-article/",
  "title": "Sample News Article Title",
  "content": "The full article content extracted by newspaper4k...",
  "author": "John Doe, Jane Smith",
  "published_date": "2025-07-08T10:30:00",
  "top_image": "https://www.example-news-site.com/images/article.jpg",
  "meta_data": {
    "viewport": "width=device-width, initial-scale=1",
    "author": "John Doe",
    "description": "Article description from meta tags",
    "og:site_name": "Example News"
  },
  "source": "example_news",
  "project": "MyProject",
  "scraped_at": "2025-07-12T15:45:00",
  "extracted_at": "2025-07-12T15:45:00"
}
```

## Analysis Tools

### 1. Inspector Tool

```bash
# Analyze single URL
bin/inspector --url https://www.example-news-site.com/2025/07/08/some-article/
# Creates analysis files in data/example_news/analysis/
```

### 2. Sitemap Discovery

```python
from core.sitemap import SitemapDiscovery

discovery = SitemapDiscovery('https://www.example-news-site.com')
sitemaps = discovery.discover_sitemaps()
article_urls = discovery.get_all_article_urls()[:10]
```

### 3. Browser Client (for JavaScript sites)

```python
from utils.browser import BrowserClient

browser = BrowserClient()
html = browser.get_rendered_html('https://www.example-news-site.com')
```

## IMPORTANT CONSTRAINTS FOR CLAUDE CODE

**Decision Making Philosophy:**

- ğŸ”„ **Be Liberal by Default**: For any spiders and parsers, be liberal in implementation choices unless explicitly told by users
- ğŸš« **Minimal Restrictions**: Don't make too many decisions or impose unnecessary constraints
- ğŸ‘¤ **User-Driven**: Only apply specific restrictions when explicitly requested by users

**âš ï¸ CRITICAL: What "Liberal by Default" Means:**

- âŒ **DON'T** filter by date ranges (like `/2024/`, `/2023/`) unless user specifies
- âŒ **DON'T** limit content types arbitrarily 
- âŒ **DON'T** restrict URL patterns beyond basic article detection
- âŒ **DON'T** add unnecessary conditions or filters
- âŒ **DON'T** use sitemap crawling unless user explicitly requests it
- âœ… **DO** collect ALL available content from the site
- âœ… **DO** use broad URL patterns that capture maximum content
- âœ… **DO** let users specify restrictions if they want them
- âœ… **DO** focus on homepage crawling for article discovery

**âš ï¸ SYSTEMATIC APPROACH: Liberal with Website-Specific Deny Rules**

Follow this systematic approach for creating spiders:

1. **ALLOW ALL by default** - No restrictive allow patterns
2. **INSPECT the actual website** - Look at real URL patterns from inspector and collected data
3. **CREATE SERIOUS DENY RULES** - Based on actual website structure, not guessing
4. **WEBSITE-SPECIFIC RULES** - Each site has unique patterns, don't copy/paste

**Common deny patterns (but always inspect first):**
- âŒ **DENY**: `/about/`, `/contact/`, `/donate/`, `/privacy/`, `/terms/`, `/advertise/`, `/careers/`, `/subscribe/`, `/help/`, `/support/`, `/login/`, `/register/`
- âŒ **DENY**: Navigation pages like `/category/$`, `/tag/$`, `/author/$`, `/search/$`
- âŒ **DENY**: Archive pages like `/news/$`, `/articles/$` (but allow `/news/article-slug/`)
- âœ… **INSPECT**: `data/website/urls.json` and `data/website/analysis/page.html` to identify patterns
- âœ… **SYSTEMATIC**: Never guess patterns - always base on real website inspection

**Examples of Liberal vs Restrictive Code:**

```python
# âŒ RESTRICTIVE (Don't do this unless user asks)
Rule(LinkExtractor(allow=r'/fact-check/[^/]+/$'), callback='parse_article', follow=True)

# âœ… LIBERAL (Default approach - allow all, deny only static pages)
Rule(LinkExtractor(
    deny=r'/(about|contact|donate|privacy|terms|advertise|careers|subscribe)/?$'
), callback='parse_article', follow=True)
```

```python
# âŒ RESTRICTIVE 
Rule(LinkExtractor(allow=r'/\d{4}/\d{2}/\d{2}/[^/]+/$'), callback='parse_article')

# âœ… LIBERAL
Rule(LinkExtractor(
    deny=r'/(about|contact|donate|help|support|login)/?$'
), callback='parse_article', follow=True)
```

**What Claude Code CAN create/modify:**

- âœ… `spiders/website.py` (domain-specific spiders using shared parser)
- âœ… `projects/X/config.yaml` (project configurations)
- âœ… Domain-specific data files in `data/website/`
- âœ… `utils/newspaper_parser.py` (shared parsing logic - already created)

**What Claude Code CANNOT modify:**

- âŒ `core/` directory files (framework code)
- âŒ `utils/` directory files (utility functions) - except newspaper_parser.py
- âŒ `bin/` directory files (CLI tools)
- âŒ `settings.py`, `scrapy.cfg` (framework config)
- âŒ Create new custom execution scripts
- âŒ Modify existing framework functionality

**Testing approach:**

- Use existing `./scrapai test` commands
- Use existing inspector tools
- No custom testing scripts

## Benefits of This System

### Code Reuse

- Write parser once in `parsers/`
- Use across multiple projects
- newspaper4k handles content extraction universally

### Client Isolation

- Separate outputs per project
- Independent configurations
- Isolated logging

### Reliability

- newspaper4k's ML-based extraction
- Handles site updates automatically
- Consistent output format

### Easy Management

- Track crawls by client
- Different spider combinations per project
- Project-specific settings

### Scalability

- Add new clients easily
- Organize hundreds of websites
- Clear billing/usage tracking

## Common Patterns

### News/Article Sites

- Create URL-collecting spider first
- Use newspaper4k for content extraction
- Add custom metadata extraction only if needed
- Test thoroughly before deployment

### Crawl Strategy

- **Homepage Crawling**: Default approach - follow navigation links from homepage
- **Sitemap Spider**: Only when user explicitly requests sitemap crawling
- **Mixed Strategy**: Use both only if user specifically asks for sitemap + crawling

## Troubleshooting

### Project Issues

- **Project not found**: Use `./scrapai projects list` to check available projects
- **Spider not configured**: Check project's `config.yaml` file
- **No outputs**: Check `projects/[name]/outputs/` directory

### Spider Issues

- **No Articles Found**: Check URL collection phase first
- **Poor Content Extraction**: newspaper4k usually handles this automatically
- **Missing Metadata**: Add custom extraction to parser if needed
- **JavaScript-Heavy Sites**: Use BrowserClient for proper rendering

### Parser Issues

- **Empty Content**: Check if newspaper4k can access the URL
- **Missing Authors/Dates**: newspaper4k extracts these automatically, add custom logic only if needed
- **Proxy Issues**: Pass proxy configuration to newspaper4k

### Permission/Access

- **Spider not found**: Ensure spider exists in `spiders/` directory
- **Parser not found**: Ensure parser exists in `parsers/` directory
- **Config errors**: Validate YAML syntax in project config
- **Output permission**: Check write access to project directories
