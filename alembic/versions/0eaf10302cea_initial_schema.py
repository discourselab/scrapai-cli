"""Initial schema

Revision ID: 0eaf10302cea
Revises: 
Create Date: 2025-12-05 01:57:47.351204

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0eaf10302cea'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema - Create initial tables."""
    # Note: This migration was auto-generated with upgrade/downgrade swapped.
    # The actual current schema is managed by core/models.py
    # This migration is kept for compatibility but does nothing
    # as setup uses Base.metadata.create_all() directly
    pass


def downgrade() -> None:
    """Downgrade schema - Drop all tables."""
    # Note: This migration was auto-generated with upgrade/downgrade swapped.
    # Downgrade not supported for initial migration
    pass


def _old_upgrade() -> None:
    """Old broken upgrade - kept for reference."""
    # This was trying to DROP tables in upgrade, which is wrong
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('sites',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('sites_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('domain', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('status', sa.VARCHAR(length=20), autoincrement=False, nullable=False),
    sa.Column('extraction_strategy', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('extraction_settings', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('crawler_settings', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('crawl_rules', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('project_assignments', postgresql.ARRAY(sa.VARCHAR()), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_successful_crawl', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('success_rate', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),
    sa.Column('total_attempts', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('successful_extractions', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name='sites_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_index(op.f('ix_sites_id'), 'sites', ['id'], unique=False)
    op.create_index(op.f('ix_sites_domain'), 'sites', ['domain'], unique=True)
    op.create_table('extraction_failures',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('site_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('url', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('extractor_attempted', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('error_type', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('error_message', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('response_status', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('retry_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('next_retry_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('failed_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['site_id'], ['sites.id'], name=op.f('extraction_failures_site_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('extraction_failures_pkey'))
    )
    op.create_index(op.f('ix_extraction_failures_id'), 'extraction_failures', ['id'], unique=False)
    op.create_table('extraction_results',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('site_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('url', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('extractor_used', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('extraction_confidence', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),
    sa.Column('extraction_duration_ms', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('title', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('content', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('published_date', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('author', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('extra_metadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('quality_score', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),
    sa.Column('content_length', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('last_crawled', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('last_updated', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.Column('content_hash', sa.VARCHAR(length=64), autoincrement=False, nullable=True),
    sa.Column('extracted_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['site_id'], ['sites.id'], name=op.f('extraction_results_site_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('extraction_results_pkey')),
    sa.UniqueConstraint('url', name=op.f('extraction_results_url_key'), postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    op.create_index(op.f('ix_extraction_results_id'), 'extraction_results', ['id'], unique=False)
    # ### end Alembic commands ###
