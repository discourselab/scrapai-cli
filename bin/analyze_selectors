#!/usr/bin/env python3
"""
Analyze HTML to discover CSS selectors for custom extraction.

Usage:
    # Analyze structure and suggest selectors
    bin/analyze_selectors data/example_com/analysis/page.html

    # Test a specific selector
    bin/analyze_selectors data/example_com/analysis/page.html --test "h1.title"

    # Find elements by keyword
    bin/analyze_selectors data/example_com/analysis/page.html --find "price"
    bin/analyze_selectors data/example_com/analysis/page.html --find "rating"
"""

import sys
import argparse
from bs4 import BeautifulSoup


def analyze_html(html_path):
    """Analyze HTML and suggest selectors."""
    with open(html_path, 'r', encoding='utf-8') as f:
        html = f.read()

    soup = BeautifulSoup(html, 'lxml')

    print(f"üìÑ Analyzing: {html_path}")
    print(f"üìä HTML size: {len(html)} bytes")
    print("\nüí° TIP: Use --find 'keyword' to search for specific elements (price, rating, etc.)\n")

    # Analyze titles
    print("=" * 60)
    print("üè∑Ô∏è  HEADERS (h1, h2) - Often used for: title, product_name, thread_title")
    print("=" * 60)
    for tag in ['h1', 'h2']:
        elements = soup.find_all(tag)
        if elements:
            print(f"\n{tag.upper()} - Found {len(elements)}:")
            for i, el in enumerate(elements[:5], 1):
                classes = el.get('class', [])
                class_str = '.' + '.'.join(classes) if classes else ''
                text = el.get_text(strip=True)[:80]
                print(f"  [{i}] {tag}{class_str}")
                print(f"      Text: {text}")

    # Analyze content containers
    print("\n" + "=" * 60)
    print("üìù CONTENT CONTAINERS (article, div with content-related classes)")
    print("=" * 60)
    content_keywords = ['article', 'content', 'body', 'text', 'post', 'entry']
    found_containers = []

    for el in soup.find_all(['article', 'div', 'section', 'main']):
        classes = el.get('class', [])
        class_str = ' '.join(classes) if classes else ''

        # Check if likely content container
        if el.name == 'article' or any(kw in class_str.lower() for kw in content_keywords):
            text_len = len(el.get_text(strip=True))
            if text_len > 200:  # Substantial content
                found_containers.append((el, text_len))

    # Sort by text length (largest first)
    found_containers.sort(key=lambda x: x[1], reverse=True)

    for i, (el, text_len) in enumerate(found_containers[:5], 1):
        classes = el.get('class', [])
        class_str = '.' + '.'.join(classes) if classes else ''
        print(f"\n  [{i}] {el.name}{class_str}")
        print(f"      Size: {text_len} chars")
        print(f"      Preview: {el.get_text(strip=True)[:80]}...")

    # Analyze dates
    print("\n" + "=" * 60)
    print("üìÖ DATES (time, elements with date/time classes)")
    print("=" * 60)
    date_keywords = ['date', 'time', 'published', 'posted', 'updated']
    found = 0

    for el in soup.find_all(['time', 'span', 'div', 'p']):
        classes = el.get('class', [])
        class_str = ' '.join(classes) if classes else ''

        if el.name == 'time' or any(kw in class_str.lower() for kw in date_keywords):
            text = el.get_text(strip=True)
            if text and len(text) < 50:  # Reasonable date length
                classes_list = el.get('class', [])
                selector = '.' + '.'.join(classes_list) if classes_list else ''
                print(f"  {el.name}{selector}: {text}")
                found += 1
                if found >= 5:
                    break

    # Analyze authors
    print("\n" + "=" * 60)
    print("‚úçÔ∏è  AUTHORS (elements with author/byline classes)")
    print("=" * 60)
    author_keywords = ['author', 'byline', 'writer', 'by']
    found = 0

    for el in soup.find_all(['span', 'div', 'a', 'p']):
        classes = el.get('class', [])
        class_str = ' '.join(classes) if classes else ''

        if any(kw in class_str.lower() for kw in author_keywords):
            text = el.get_text(strip=True)
            if text and len(text) < 100:  # Reasonable author length
                classes_list = el.get('class', [])
                selector = '.' + '.'.join(classes_list) if classes_list else ''
                print(f"  {el.name}{selector}: {text}")
                found += 1
                if found >= 5:
                    break

    print("\n" + "=" * 60)


def test_selector(html_path, selector):
    """Test a CSS selector on HTML."""
    with open(html_path, 'r', encoding='utf-8') as f:
        html = f.read()

    soup = BeautifulSoup(html, 'lxml')

    print(f"\nüîç Testing selector: {selector}")
    print("=" * 60)

    elements = soup.select(selector)

    if not elements:
        print("‚ùå No elements found!")
        return

    print(f"‚úì Found {len(elements)} element(s)\n")

    for i, el in enumerate(elements[:3], 1):
        text = el.get_text(strip=True)
        print(f"[{i}] {el.name}")
        print(f"    Classes: {el.get('class', [])}")
        print(f"    Text ({len(text)} chars): {text[:150]}...")
        print()


def find_by_keyword(html_path, keyword):
    """Find elements by keyword in classes or text."""
    with open(html_path, 'r', encoding='utf-8') as f:
        html = f.read()

    soup = BeautifulSoup(html, 'lxml')

    print(f"\nüîé Finding elements with keyword: '{keyword}'")
    print("=" * 60)

    found = 0

    # Search in all elements
    for el in soup.find_all():
        # Check classes
        classes = el.get('class', [])
        class_str = ' '.join(classes) if classes else ''

        # Check id
        el_id = el.get('id', '')

        # Match keyword in class or id
        if keyword.lower() in class_str.lower() or keyword.lower() in el_id.lower():
            text = el.get_text(strip=True)
            if text and len(text) < 200:  # Reasonable length
                classes_list = el.get('class', [])
                selector = '.' + '.'.join(classes_list) if classes_list else ''
                if el_id:
                    selector = f"#{el_id}"
                print(f"\n  {el.name}{selector}")
                print(f"    Text: {text[:100]}")
                found += 1
                if found >= 10:
                    break

    if found == 0:
        print(f"\n‚ùå No elements found with keyword '{keyword}'")
        print("\nüí° Try searching for:")
        print("   - Class keywords: 'price', 'rating', 'author', 'date', 'title'")
        print("   - Content keywords: look at text content in analyze mode")


def main():
    parser = argparse.ArgumentParser(description='Analyze HTML for CSS selector discovery')
    parser.add_argument('html_file', help='Path to HTML file to analyze')
    parser.add_argument('--test', help='Test a specific CSS selector')
    parser.add_argument('--find', help='Find elements by keyword (searches classes/ids)')

    args = parser.parse_args()

    if args.test:
        test_selector(args.html_file, args.test)
    elif args.find:
        find_by_keyword(args.html_file, args.find)
    else:
        analyze_html(args.html_file)


if __name__ == '__main__':
    main()
