#!/usr/bin/env python3
"""
collect - Unified tool for running crawler and scraper in sequence

This script provides a unified interface for running both the crawler (URL discovery)
and scraper (content extraction) in a single command. It automates the entire
data collection process for a specified source.
"""
import os
import sys
import argparse
import subprocess
import logging
from time import sleep

# Add the parent directory to the path so we can import from utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logger import get_logger, setup_logging

# Use a custom format for the collect script that makes output cleaner
logger = setup_logging('collect', log_format='%(asctime)s - %(levelname)s - %(message)s')

def parse_args():
    parser = argparse.ArgumentParser(
        description='Run crawler and scraper in sequence for a specific source',
        epilog='Example: collect --source skepticalscience --max-pages 10 --limit 20'
    )
    
    # Required arguments
    parser.add_argument('--source', '-s', required=True, help='Source ID to process')
    
    # Crawler options
    parser.add_argument('--max-pages', '-mp', type=int, help='Maximum number of pages to crawl')
    parser.add_argument('--new-only', '-n', action='store_true', help='Only crawl for new articles')
    parser.add_argument('--overwrite', '-o', action='store_true', help='Overwrite existing articles when crawling')
    
    # Scraper options
    parser.add_argument('--limit', '-l', type=int, help='Limit the number of articles to scrape')
    parser.add_argument('--force', '-f', action='store_true', help='Force scraping of all articles, including already scraped ones')
    
    # Other options
    parser.add_argument('--crawler-only', action='store_true', help='Only run the crawler, not the scraper')
    parser.add_argument('--scraper-only', action='store_true', help='Only run the scraper, not the crawler')
    parser.add_argument('--delay', '-d', type=int, default=1, help='Delay in seconds between crawler and scraper (default: 1)')
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    logger.info(f"‚ú® Starting data collection for source: {args.source} ‚ú®")
    
    # Get the full path to the bin directory
    bin_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Run the crawler if not in scraper-only mode
    if not args.scraper_only:
        crawler_cmd = [os.path.join(bin_dir, 'crawler'), '--source', args.source]
        
        # Add optional crawler arguments
        if args.max_pages is not None:
            crawler_cmd.extend(['--max-pages', str(args.max_pages)])
        if args.new_only:
            crawler_cmd.append('--new-only')
        if args.overwrite:
            crawler_cmd.append('--overwrite')
            
        logger.info(f"Running crawler: {' '.join(crawler_cmd)}")
        
        # Run the crawler with the virtual environment's Python
        venv_python = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.venv', 'bin', 'python')
        crawler_cmd = [venv_python] + crawler_cmd
        crawler_process = subprocess.run(crawler_cmd, capture_output=False, text=True)
        
        # Check if crawler succeeded
        if crawler_process.returncode != 0:
            logger.error(f"‚ùå Crawler failed with exit code {crawler_process.returncode}")
            return crawler_process.returncode
            
        logger.info(f"‚úÖ Crawler completed successfully for {args.source}")
        
        # Add a delay between crawler and scraper to let file system operations complete
        if not args.crawler_only and args.delay > 0:
            logger.info(f"‚è±Ô∏è  Waiting {args.delay} seconds before running scraper...")
            sleep(args.delay)
    
    # Run the scraper if not in crawler-only mode
    if not args.crawler_only:
        scraper_cmd = [os.path.join(bin_dir, 'scraper'), '--source', args.source]
        
        # Add optional scraper arguments
        if args.limit is not None:
            scraper_cmd.extend(['--limit', str(args.limit)])
        if args.force:
            scraper_cmd.append('--force')
            
        logger.info(f"Running scraper: {' '.join(scraper_cmd)}")
        
        # Run the scraper with the virtual environment's Python
        venv_python = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.venv', 'bin', 'python')
        scraper_cmd = [venv_python] + scraper_cmd
        scraper_process = subprocess.run(scraper_cmd, capture_output=False, text=True)
        
        # Check if scraper succeeded
        if scraper_process.returncode != 0:
            logger.error(f"‚ùå Scraper failed with exit code {scraper_process.returncode}")
            return scraper_process.returncode
            
        logger.info(f"‚úÖ Scraper completed successfully for {args.source}")
    
    logger.info(f"üéâ Data collection completed successfully for {args.source} üéâ")
    return 0

if __name__ == '__main__':
    sys.exit(main())